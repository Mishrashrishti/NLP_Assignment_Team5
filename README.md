## Dataset Curation, Cleaning, Deduplication, Tokenization and Fine-Tuning LM for Language Processing

This repository contains the code and resources for curating, cleaning, and deduplicating a language dataset. The project involves collecting data from publicly accessible sources, removing inappropriate content using custom-developed and existing bad-word dictionaries, and deduplicating the dataset using multiple robust techniques. The cleaned and deduplicated dataset is then used to train multiple tokenizer. The best tokenizer is thus used for tokenizing the dataset which is at last used for fine tuning a language model. Detailed statistics and analysis provided in the reports.

Link for assignment 1 report - https://docs.google.com/document/d/19XPgk-LuuJt1TgOKGrEAA90_nVv8hYzCNIdbVmdoyg8/edit?usp=sharing





