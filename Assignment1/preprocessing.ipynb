{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: chardet in /Users/heerkubadia/Library/Python/3.9/lib/python/site-packages (5.2.0)\n",
      "Requirement already satisfied: nltk in /Users/heerkubadia/Library/Python/3.9/lib/python/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /Users/heerkubadia/Library/Python/3.9/lib/python/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/heerkubadia/Library/Python/3.9/lib/python/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/heerkubadia/Library/Python/3.9/lib/python/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /Users/heerkubadia/Library/Python/3.9/lib/python/site-packages (from nltk) (4.66.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install chardet nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a dictionary of bad words using washyourmouthoutwithsoap which is a module that provides a list of bad words and various internet sources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words = [\n",
    "    \"ഗർഭഛിദ്രം\",\n",
    "    \"മലദ്വാരം\",\n",
    "    \"കഴുത\",\n",
    "    \"അസി ഫക്കർ\",\n",
    "    \"കഴുതകളെ\",\n",
    "    \"ആസ്ഹോൾ\",\n",
    "    \"അശ്ളീലങ്ങൾ\",\n",
    "    \"ബോൾബാഗ്\",\n",
    "    \"പന്തുകൾ\",\n",
    "    \"തന്തയില്ലാത്തവൻ\",\n",
    "    \"ബെല്ലെൻഡ്\",\n",
    "    \"മൃഗീയമായ\",\n",
    "    \"മൃഗീയത\",\n",
    "    \"ബിച്ച്\",\n",
    "    \"ബിച്ചുകൾ\",\n",
    "    \"ബിപിഡിംഗ്\",\n",
    "    \"രക്തരൂക്ഷിതമായ\",\n",
    "    \"ബലോക്ക്\",\n",
    "    \"ബോബ്\",\n",
    "    \"പൂക്കൾ\",\n",
    "    \"സ്തനങ്ങൾ\",\n",
    "    \"ബ്യൂട്ടാ\",\n",
    "    \"ബം\",\n",
    "    \"മയക്കുമരുന്ന്\",\n",
    "    \"പരവതാനി മാൻച്ചർ\",\n",
    "    \"ചുംബ്\",\n",
    "    \"സിപാ\",\n",
    "    \"ക്ലോറിസിസ്\",\n",
    "    \"കോക്ക്\",\n",
    "    \"കോക്ക് സക്കർ\",\n",
    "    \"കോക്സ്\",\n",
    "    \"കോൺ\",\n",
    "    \"ക്രാപ്പ്\",\n",
    "    \"ശുക്ലം\",\n",
    "    \"പുരുഷാരം\",\n",
    "    \"ഷിറ്റ്\",\n",
    "    \"പിസ്സ്\",\n",
    "    \"ഫക്ക്\",\n",
    "    \"കൻ്റ്\",\n",
    "    \"കോക്ക് സക്കർ\",\n",
    "    \"മദർഫക്കർ\",\n",
    "    \"ടിറ്റ്സ്\",\n",
    "    \"മുഷിഞ്ഞ\",\n",
    "    \"കഷ്ടം\",\n",
    "    \"ഡിക്ക്\",\n",
    "    \"ഡിൽഡോ\",\n",
    "    \"നായ-ഫക്കർ\",\n",
    "    \"ഫാനി\",\n",
    "    \"ഊമ്പി\",\n",
    "    \"സംഭോഗം ചെയ്യുക\",\n",
    "    \"ഫക്കർ\",\n",
    "    \"ഫഡ്ജ് പാക്കർ\",\n",
    "    \"ദൈവം-കൊള്ളിത\",\n",
    "    \"ഗോഡ്ഡം\",\n",
    "    \"നരകം\",\n",
    "    \"വയ്ക്കുക\",\n",
    "    \"വൃത്തികെട്ട\",\n",
    "    \"ജെർക് ഓഫ്\",\n",
    "    \"കിക്ക്\",\n",
    "    \"ലാബിയ\",\n",
    "    \"മോഹം\",\n",
    "    \"മോഹഭംഗം\",\n",
    "    \"മാസോച്ചിസ്റ്റ്\",\n",
    "    \"സ്വയംഭോഗം ചെയ്യുക\",\n",
    "    \"അമ്മ ഫക്കർ\",\n",
    "    \"നാസി\",\n",
    "    \"നിഗർ\",\n",
    "    \"മയക്കുമരുന്നുകൾ\",\n",
    "    \"രതിമൂർച്ഛ\",\n",
    "    \"പെക്കർ\",\n",
    "    \"ലിംഗം\",\n",
    "    \"മൂത്രമൊഴിക്കുക\",\n",
    "    \"പിസ്സർ\",\n",
    "    \"പിസ്സകൾ\",\n",
    "    \"പിസ്സോഫ്\",\n",
    "    \"അശ്ലീലം\",\n",
    "    \"അശ്ലീലത\",\n",
    "    \"പ്രാവി\",\n",
    "    \"വിസർജ്യങ്ങൾ\",\n",
    "    \"പ്യൂബ്\",\n",
    "    \"ബലാൽസംഗം\",\n",
    "    \"ബലാത്സംഗം\",\n",
    "    \"മലാശയം\",\n",
    "    \"റിമ്മിംഗ്\",\n",
    "    \"സചിസ്റ്റ്\",\n",
    "    \"പുല്ല്\",\n",
    "    \"ബീജം\",\n",
    "    \"ശവം\",\n",
    "    \"ഷാഗിംഗ്\",\n",
    "    \"ഷീറ്റ്\",\n",
    "    \"ഷെയ്റ്റ്\",\n",
    "    \"ഷൈറ്റി\",\n",
    "    \"മന്ദഹസരം\",\n",
    "    \"സ്നെഗമാ\",\n",
    "    \"വെറുക്കപ്പെട്ടയാൾ\",\n",
    "    \"സ്പെയ്ക്\",\n",
    "    \"തുളച്ച്\",\n",
    "    \"വൃഷണം\",\n",
    "    \"പേപട്ട\",\n",
    "    \"യോനി\",\n",
    "    \"വരാഗ്ര\",\n",
    "    \"വാൽവ\",\n",
    "    \"വേശ്യ\",\n",
    "    \"റേറ്റുചെയ്തു\",\n",
    "    \"തേവിഡിയ\",\n",
    "    \"വേശ്യ\",\n",
    "    \"പുണ്ടൈ\",\n",
    "    \"കുണ്ണ\",\n",
    "    \"പൂളു\",\n",
    "    \"ഡിക്ക്\",\n",
    "    \"ഓത\",\n",
    "    \"കൊള്ളൂ\",\n",
    "    \"ഓളു\",\n",
    "    \"ഫക്കിംഗിൻ്റെ പ്രവൃത്തി\",\n",
    "    \"സുന്നി\",\n",
    "    \"ഫോറിൻ / ഡിക്ക്\",\n",
    "    \"ബാദു\",\n",
    "    \"പിമ്പ്\",\n",
    "    \"ഒമ്മാള\",\n",
    "    \"ഊമ്പു\",\n",
    "    \"ഊതുക\",\n",
    "    \"കൂടി\",\n",
    "    \"കുണ്ണ\",\n",
    "    \"സൂതു\",\n",
    "    \"കഴുത\",\n",
    "    \"ലവദേകബാൽ\",\n",
    "    \"സപ്പു\",\n",
    "    \"നക്കി\",\n",
    "    \"കായ് മുല\",\n",
    "    \"കൂട്ടി കുടു പിമ്പിംഗ് ഔട്ട്\",\n",
    "    \"മുണ്ടൈ വിധവ\",\n",
    "    \"നിൻ്റെ അമ്മ, പതി!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below provided code is designed to process text files in a given folder, identify and remove sentences containing specified \"bad words\" or phrases, and save the cleaned version of these files. It also logs the sentences that were removed along with the offending words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged removed sentences for /Users/heerkubadia/Desktop/Sem - 5/Natural Language Processing/nlp_data/MADLAD-400/ml_clean_0000_part_04.txt in /Users/heerkubadia/Desktop/Sem - 5/Natural Language Processing/nlp_data_removed_sentences_logs/ml_clean_0000_part_04.txt_removed_log.txt.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import chardet\n",
    "import datetime\n",
    "\n",
    "# Escape special regex characters in bad words\n",
    "escaped_bad_words = [re.escape(word) for word in bad_words]\n",
    "\n",
    "# Sort bad words by length in descending order to handle longer phrases first\n",
    "escaped_bad_words.sort(key=lambda x: len(x), reverse=True)\n",
    "\n",
    "# Create a regex pattern with word boundaries for whole word/phrase matching\n",
    "# Use non-capturing group (?:...) for efficiency\n",
    "bad_words_pattern = re.compile(\n",
    "    r'(?<!\\w)(' + '|'.join(escaped_bad_words) + r')(?!\\w)',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "\n",
    "def detect_encoding(file_path, sample_size=100000):\n",
    "    \"\"\"Detect the encoding of a file using chardet.\"\"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        raw_data = file.read(sample_size)  # Read a chunk of the file to analyze\n",
    "        result = chardet.detect(raw_data)\n",
    "        return result['encoding']\n",
    "\n",
    "def bad_word_in_sentence(sentence, bad_words_pattern):\n",
    "    \"\"\"Check if any bad word is present in the sentence and return the bad word(s).\"\"\"\n",
    "    found_words = bad_words_pattern.findall(sentence)\n",
    "    return list(set(found_words)) if found_words else None\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    \"\"\"\n",
    "    Split Malayalam text into sentences using regular expressions.\n",
    "    Malayalam uses different punctuation marks for sentence boundaries.\n",
    "    Common sentence terminators include: '.', '!', '?', '।', etc.\n",
    "    \"\"\"\n",
    "    # Define Malayalam sentence terminators\n",
    "    sentence_endings = r'[.!?।]'\n",
    "    \n",
    "    # Use regex to split text into sentences based on the defined terminators\n",
    "    sentences = re.split(sentence_endings, text)\n",
    "    \n",
    "    # Remove any leading/trailing whitespace and filter out empty sentences\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def log_removed_sentences(file_path, removed_sentences, log_folder_path):\n",
    "    \"\"\"Log the sentences that were removed along with the bad words that caused their removal.\"\"\"\n",
    "    \n",
    "    # Ensure the log folder exists\n",
    "    os.makedirs(log_folder_path, exist_ok=True)\n",
    "    \n",
    "    # Create the log file path in the log folder\n",
    "    base_filename = os.path.basename(file_path)  # Extract the original file name\n",
    "    log_file_name = base_filename + \"_removed_log.txt\"  # Append the suffix\n",
    "    log_file_path = os.path.join(log_folder_path, log_file_name)  # Construct full log file path\n",
    "    \n",
    "    if removed_sentences:\n",
    "        with open(log_file_path, 'a', encoding='utf-8', errors='ignore') as log_file:\n",
    "            log_file.write(f\"\\n[{datetime.datetime.now()}] Removed sentences from {file_path}:\\n\")\n",
    "            for sentence, bad_words in removed_sentences:\n",
    "                log_file.write(f\"- Sentence: {sentence}\\n\")\n",
    "                log_file.write(f\"  Bad word(s): {', '.join(bad_words)}\\n\")\n",
    "        print(f\"Logged removed sentences for {file_path} in {log_file_path}.\")\n",
    "\n",
    "def process_text_file(input_file_path, output_file_path, bad_words_pattern, log_folder_path):\n",
    "    \"\"\"Process a single text file to remove sentences with bad words and save it to the output path.\"\"\"\n",
    "    encoding = detect_encoding(input_file_path)\n",
    "    \n",
    "    if encoding is None:\n",
    "        encoding = 'utf-8'  # Fallback to utf-8 if no encoding is detected\n",
    "    \n",
    "    try:\n",
    "        with open(input_file_path, 'r', encoding=encoding, errors='ignore') as file:\n",
    "            text = file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"UnicodeDecodeError: Unable to read file {input_file_path}. Skipping this file.\")\n",
    "        return\n",
    "\n",
    "    sentences = split_into_sentences(text)\n",
    "    filtered_sentences = []\n",
    "    removed_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        bad_word_list = bad_word_in_sentence(sentence, bad_words_pattern)\n",
    "        if bad_word_list:\n",
    "            removed_sentences.append((sentence, bad_word_list))\n",
    "        else:\n",
    "            filtered_sentences.append(sentence)\n",
    "\n",
    "    new_text = ' '.join(filtered_sentences)\n",
    "\n",
    "    with open(output_file_path, 'w', encoding='utf-8', errors='ignore') as file:\n",
    "        file.write(new_text)\n",
    "\n",
    "    # Log removed sentences\n",
    "    log_removed_sentences(input_file_path, removed_sentences, log_folder_path)\n",
    "\n",
    "def process_folder(input_folder_path, output_folder_path, log_folder_path, bad_words_pattern):\n",
    "    \"\"\"Traverse through the folder structure, process all text files, and save them in the output folder.\"\"\"\n",
    "    for root, dirs, files in os.walk(input_folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                # Build input file path\n",
    "                input_file_path = os.path.join(root, file)\n",
    "                \n",
    "                # Build corresponding output file path\n",
    "                relative_path = os.path.relpath(input_file_path, input_folder_path)\n",
    "                output_file_path = os.path.join(output_folder_path, relative_path)\n",
    "                \n",
    "                # Create directories in the output folder if they don't exist\n",
    "                os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n",
    "                \n",
    "                # Process and save the file\n",
    "                process_text_file(input_file_path, output_file_path, bad_words_pattern, log_folder_path)\n",
    "\n",
    "# Path to the input dataset folder\n",
    "input_folder_path = \"/Users/heerkubadia/Desktop/Sem - 5/Natural Language Processing/nlp_data\"\n",
    "\n",
    "# Path to the output dataset folder\n",
    "output_folder_path = \"/Users/heerkubadia/Desktop/Sem - 5/Natural Language Processing/nlp_data_filtered\"\n",
    "\n",
    "# Path to the log folder where removed sentences will be stored\n",
    "log_folder_path = \"/Users/heerkubadia/Desktop/Sem - 5/Natural Language Processing/nlp_data_removed_sentences_logs\"\n",
    "\n",
    "# Process the folder\n",
    "process_folder(input_folder_path, output_folder_path, log_folder_path, bad_words_pattern)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
