{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "miHblrp5X9eG",
        "5eE4nUslavS6",
        "yUw2dq0x7bEo",
        "NHfxWn0M0qpc"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Hi y'all, this is Team-5 and here we've documented the entire deduplication methods and pipelines utilised in our assignment."
      ],
      "metadata": {
        "id": "F4iQhY8MXQDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The primary common step of reading the input folders, writing the output folders and normalizing text"
      ],
      "metadata": {
        "id": "miHblrp5X9eG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have one parent folder which in turn stores a number of folders corresponding to each unique source. These folders in turn contain a list of .txt files. Through our deduplication methodoligies, we read through all the .txt files accross all the folders and remove the duplicate .txt files which have similarity scores above a pre-set threshold."
      ],
      "metadata": {
        "id": "u3eWn1ABYQkN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code helps in reading and storing all the text files across all the input folders; and writing the unique text files into the new output folders in the same manner."
      ],
      "metadata": {
        "id": "Ni8-ysx1ZvTR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZJH4oUaXMbY"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all, we'll load all the text files from our folder structure"
      ],
      "metadata": {
        "id": "AMZXRRA6ePyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_documents_from_folders(root_folder):\n",
        "    documents = []\n",
        "    file_paths = []\n",
        "\n",
        "    #recursively walking through all subfolders and files\n",
        "    for dirpath, _, filenames in os.walk(root_folder):\n",
        "        for filename in filenames:\n",
        "            if filename.endswith(\".txt\"):  #processing only .txt files (not required but keeping a margin, just in case)\n",
        "                file_path = os.path.join(dirpath, filename)\n",
        "                file_paths.append(file_path)\n",
        "                with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                    documents.append(file.read())\n",
        "\n",
        "    return documents, file_paths"
      ],
      "metadata": {
        "id": "S1FZhqPsaMyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we're writing the unique documents back to the output folder, while preserving our document structure as well"
      ],
      "metadata": {
        "id": "OKpvuBZYfFgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_unique_documents(unique_docs, original_file_paths, output_folder):\n",
        "    for doc in unique_docs:\n",
        "        original_file_path = original_file_paths[doc['index']]\n",
        "        # Recreate the original folder structure in the output folder\n",
        "        relative_path = os.path.relpath(original_file_path, start=input_folder)\n",
        "        output_file_path = os.path.join(output_folder, relative_path)\n",
        "        os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n",
        "\n",
        "        # Copy the original file to the new folder\n",
        "        copyfile(original_file_path, output_file_path)"
      ],
      "metadata": {
        "id": "urxbxGaVaPUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Storing the paths to our input and output parent folder, also loading the documents and file paths."
      ],
      "metadata": {
        "id": "VJ2rIF-JfiIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_folder = \"/Users/utkarshsrivastava/Downloads/parentfolder\"\n",
        "output_folder = \"/Users/utkarshsrivastava/Downloads/parentfolder_deduplicated\""
      ],
      "metadata": {
        "id": "tcJtfy2CaR71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents, file_paths = load_documents_from_folders(input_folder)"
      ],
      "metadata": {
        "id": "Zx-b7VjmaVCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we've successfully managed to input all the .txt. files"
      ],
      "metadata": {
        "id": "gd0EGmo0ahOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we'll go on with doing a basic text normalization to improve the deduplication by lowercasing and removing extra spaces."
      ],
      "metadata": {
        "id": "5HjPYct2a5ww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(text):\n",
        "    return ' '.join(text.lower().split())"
      ],
      "metadata": {
        "id": "30eczEEJak6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deduplication Pipelines"
      ],
      "metadata": {
        "id": "wysBhuiLapMY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Technique 1 - MinHash+LSH"
      ],
      "metadata": {
        "id": "5eE4nUslavS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasketch import MinHash, MinHashLSH"
      ],
      "metadata": {
        "id": "DvQ2vSp38kIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the major step comes in of creating a MinHash object and an LSH model to group similar documents"
      ],
      "metadata": {
        "id": "2AkywrQVbOPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_minhash(doc):\n",
        "    m = MinHash(num_perm=128)\n",
        "    normalized_doc = normalize_text(doc)  #Applying normalization\n",
        "    for word in normalized_doc.split():\n",
        "        m.update(word.encode('utf8'))\n",
        "    return m\n",
        "\n",
        "threshold = 0.8  #Setting similarity threshold for near-duplicate detection\n",
        "lsh = MinHashLSH(threshold=threshold, num_perm=128)\n",
        "\n",
        "#Inserting MinHashes into the LSH\n",
        "minhashes = {}\n",
        "for idx, doc in enumerate(documents):\n",
        "    m = compute_minhash(doc)\n",
        "    lsh.insert(f'doc_{idx}', m)\n",
        "    minhashes[idx] = m"
      ],
      "metadata": {
        "id": "SvmSzuVQbOpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is pretty straightforward, we'll keep a track of unique and duplicates documents and ensure that the copy of the parent folder we're creating as an output only stores the unique ones (the first occurence among the cases of duplications)"
      ],
      "metadata": {
        "id": "VzdQpsiCdRym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_docs = set()\n",
        "duplicates = set()\n",
        "\n",
        "for idx, doc in enumerate(documents):\n",
        "    m = minhashes[idx]\n",
        "\n",
        "    if f'doc_{idx}' not in unique_docs:\n",
        "        #Querying similar documents from LSH\n",
        "        similar_docs = lsh.query(m)\n",
        "        for sim_doc in similar_docs:\n",
        "            if sim_doc != f'doc_{idx}':\n",
        "                duplicates.add(sim_doc)\n",
        "        unique_docs.add(f'doc_{idx}')\n",
        "\n",
        "#Filtering out duplicates\n",
        "filtered_docs = [f'doc_{idx}' for idx in range(len(documents)) if f'doc_{idx}' not in duplicates]"
      ],
      "metadata": {
        "id": "T2r41F5Wc5sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "here we've use a slightly different function to save the unique documents in the output folder following the same folder structure as the input folder."
      ],
      "metadata": {
        "id": "zd-iTZSU_SSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_unique_documents(unique_docs, original_file_paths, output_folder):\n",
        "    for doc_id in unique_docs:\n",
        "        original_file_path = original_file_paths[int(doc_id.split('_')[1])]\n",
        "\n",
        "        #recreating the original folder structure in the output folder\n",
        "        relative_path = os.path.relpath(original_file_path, start=input_folder)\n",
        "        output_file_path = os.path.join(output_folder, relative_path)\n",
        "        os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n",
        "\n",
        "        #copying the content to the new file\n",
        "        with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
        "            out_file.write(documents[int(doc_id.split('_')[1])])\n",
        "\n",
        "save_unique_documents(filtered_docs, file_paths, output_folder)"
      ],
      "metadata": {
        "id": "VTgYv35LBXUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Deduplicated documents saved to {output_folder}\")"
      ],
      "metadata": {
        "id": "9BBlVy1Hd7ML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voila!"
      ],
      "metadata": {
        "id": "blrOhdwreIRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Technique 2: Fuzzy Matching with Levenshtein distance"
      ],
      "metadata": {
        "id": "yUw2dq0x7bEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fuzzywuzzy import fuzz\n",
        "from shutil import copyfile"
      ],
      "metadata": {
        "id": "e7d53zgJ7wKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "here we'll normalize in the beginning"
      ],
      "metadata": {
        "id": "O1r-V1SG-mYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_documents = [normalize_text(doc) for doc in documents"
      ],
      "metadata": {
        "id": "bptHhMK78vkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keeping track of unique documents, with the threshold parameter set as if the similarity score is >85, then only consider the case a duplicate."
      ],
      "metadata": {
        "id": "CJ2AJLrb-4d3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_docs = []\n",
        "duplicates = set()\n",
        "\n",
        "for idx, (doc, normalized_doc) in enumerate(zip(documents, normalized_documents)):\n",
        "    is_duplicate = False\n",
        "    for u_doc in unique_docs:\n",
        "        # Check if the document is a duplicate using fuzzy matching\n",
        "        if fuzz.ratio(normalized_doc, u_doc['normalized']) > 85:  # Set threshold for similarity\n",
        "            is_duplicate = True\n",
        "            break\n",
        "\n",
        "    if not is_duplicate:\n",
        "        unique_docs.append({'content': doc, 'normalized': normalized_doc, 'index': idx})"
      ],
      "metadata": {
        "id": "lhXg58CA-49m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the unique documents to the output folder, maintaining folder structure\n"
      ],
      "metadata": {
        "id": "sp5165f7d9Cp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_unique_documents(unique_docs, file_paths, output_folder)\n",
        "\n",
        "print(f\"Unique documents saved to {output_folder}\")"
      ],
      "metadata": {
        "id": "oKa1Trfk_SrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voila!"
      ],
      "metadata": {
        "id": "g9bPg69V_pBB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Technique 3: Cosine Similarity"
      ],
      "metadata": {
        "id": "1DHuHbTmACXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "umftxxXiAXkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "here we'll normalize in the beginning"
      ],
      "metadata": {
        "id": "ps-z_BWVBx9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_documents = [normalize_text(doc) for doc in documents"
      ],
      "metadata": {
        "id": "5UchaPBuAWA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using TF-IDF to convert documents into vectors and computing cosine similarity matrix"
      ],
      "metadata": {
        "id": "5fazfE68B1Ds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(normalized_documents)"
      ],
      "metadata": {
        "id": "_-VbQpM_B0Zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keeping track of unique documents, with the threshold parameter set as if the similarity score is >0.91, then only consider the case a duplicate."
      ],
      "metadata": {
        "id": "lWesjYiJCfsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_threshold = 0.91\n",
        "\n",
        "# Identify unique documents based on cosine similarity\n",
        "unique_docs = []\n",
        "duplicates = set()\n",
        "\n",
        "for i in range(len(documents)):\n",
        "    if i not in duplicates:\n",
        "        unique_docs.append(i)\n",
        "        for j in range(i + 1, len(documents)):\n",
        "            if cosine_sim_matrix[i, j] > similarity_threshold:\n",
        "                duplicates.add(j)"
      ],
      "metadata": {
        "id": "4OWJEQDCCg8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the unique documents to the output folder, maintaining folder structure"
      ],
      "metadata": {
        "id": "RkAxZHw_CAu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_unique_documents(unique_docs, file_paths, output_folder)\n",
        "\n",
        "print(f\"Unique documents saved to {output_folder}\")"
      ],
      "metadata": {
        "id": "86Gn2K_rCBLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voila!"
      ],
      "metadata": {
        "id": "nxBWkMvcCMkX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Technique 4: CSV Data Storage + SimHash + N-gram Sampling\n"
      ],
      "metadata": {
        "id": "NHfxWn0M0qpc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hash Generation from Text Files:\n",
        "\n",
        "->We start with a parent folder, which contains multiple subfolders. Each subfolder contains several text files.\n",
        "\n",
        "->The first step involves processing each subfolder in the parent folder. For every text file within these subfolders, we apply a hash function (SimHash or MinHash) to generate a unique hash for the content of each file.\n",
        "\n",
        "->These generated hashes, along with corresponding file names, are then written to a CSV file.\n",
        "\n",
        "Deduplication:\n",
        "\n",
        "->Use the CSV to identify duplicates based on hash values, log the duplicates, and remove them.\n",
        "\n",
        "Transfer Deduplicated Files:\n",
        "\n",
        "->Move the unique (non-duplicate) files to the output folder, ensuring only deduplicated files are stored.\n",
        "\n",
        "->We can enhance the deduplication process by incorporating more robust techniques to compare text files across folders"
      ],
      "metadata": {
        "id": "Jzu3iyhlwMFS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Use Simhash or MinHash"
      ],
      "metadata": {
        "id": "Idu_g_w6zSwX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for Simhash"
      ],
      "metadata": {
        "id": "G5Fwj0wY0Z0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from simhash import Simhash\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from multiprocessing import Pool, cpu_count\n",
        "\n",
        "\n",
        "def features_extract(s):\n",
        "    width = 3\n",
        "    s = s.lower()\n",
        "    s = re.sub(r'[^\\w]+', '', s)\n",
        "    return [s[i:i + width] for i in range(max(len(s) - width + 1, 1))]\n",
        "\n",
        "\n",
        "def compute_simhash(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "    simhash_value = Simhash(features_extract(content))\n",
        "    return file_path, simhash_value.value\n",
        "\n",
        "\n",
        "def process_directory(args):\n",
        "    path, parent = args\n",
        "    data = []\n",
        "    relative_path = os.path.relpath(path, parent)\n",
        "\n",
        "    # Skip processing if the output CSV already exists\n",
        "    if os.path.exists(output_file):\n",
        "        print(f\"Skipping {path}, CSV already exists\")\n",
        "        return data\n",
        "\n",
        "    s = relative_path.replace(os.sep, '_')\n",
        "    for file in tqdm(os.listdir(path), leave=False, desc=s):\n",
        "        file_path = os.path.join(path, file)\n",
        "        if os.path.isfile(file_path):\n",
        "            file_path, simhash_value = compute_simhash(file_path)\n",
        "            data.append({\n",
        "                \"file\": file_path,\n",
        "                \"hash\": simhash_value\n",
        "            })\n",
        "    if data:\n",
        "        df = pd.DataFrame(data)\n",
        "        df.to_csv(f\"path where csv needs to be stored{s}.csv\", index=False)   #give a path where csv needs to be stored koi,  where the hash csvs are stored\n",
        "    return data\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parent = \"Parent folder path\"     #sara data daal do ek parent folder mein aur idher path daal dena\n",
        "    directories = [(os.path.join(path, d), parent) for path, dirs, files in os.walk(parent) for d in dirs]\n",
        "\n",
        "    # Use all available CPUs (one cpu per csv)\n",
        "    with Pool(cpu_count()) as pool:\n",
        "        pool.map(process_directory, directories)"
      ],
      "metadata": {
        "id": "xN61_PhlyKWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Code for MinHash"
      ],
      "metadata": {
        "id": "pdbes9Xt0WTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from datasketch import MinHash, MinHashLSH  # Import MinHash and LSH from datasketch\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from multiprocessing import Pool, cpu_count\n",
        "\n",
        "\n",
        "def features_extract(s):\n",
        "    \"\"\"Generate n-grams from the input string.\"\"\"\n",
        "    width = 3\n",
        "    s = s.lower()\n",
        "    s = re.sub(r'[^\\w]+', '', s)\n",
        "    return [s[i:i + width] for i in range(max(len(s) - width + 1, 1))]\n",
        "\n",
        "\n",
        "def compute_minhash(file_path):\n",
        "    \"\"\"Compute MinHash signature for a file.\"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    # Generate n-grams\n",
        "    ngrams = features_extract(content)\n",
        "\n",
        "    # Create a MinHash object and update it with the n-grams\n",
        "    minhash = MinHash()\n",
        "    for ngram in ngrams:\n",
        "        minhash.update(ngram.encode('utf-8'))  # Update with the byte representation of n-grams\n",
        "\n",
        "    return file_path, minhash\n",
        "\n",
        "\n",
        "def process_directory(args):\n",
        "    \"\"\"Process a directory of files and compute their MinHash signatures.\"\"\"\n",
        "    path, parent = args\n",
        "    data = []\n",
        "    relative_path = os.path.relpath(path, parent)\n",
        "\n",
        "    # Skip processing if the output CSV already exists\n",
        "    if os.path.exists(output_file):\n",
        "        print(f\"Skipping {path}, CSV already exists\")\n",
        "        return data\n",
        "\n",
        "    s = relative_path.replace(os.sep, '_')\n",
        "    for file in tqdm(os.listdir(path), leave=False, desc=s):\n",
        "        file_path = os.path.join(path, file)\n",
        "        if os.path.isfile(file_path):\n",
        "            file_path, minhash = compute_minhash(file_path)\n",
        "            data.append({\n",
        "                \"file\": file_path,\n",
        "                \"hash\": minhash,  # Store MinHash object\n",
        "                \"hash_values\": minhash.hash  # Optionally store the actual hash values\n",
        "            })\n",
        "\n",
        "    # Create DataFrame and save to CSV\n",
        "    if data:\n",
        "        df = pd.DataFrame(data)\n",
        "        df.to_csv(f\"path where csv needs to be stored/{s}.csv\", index=False)  # Specify the output path\n",
        "    return data\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parent = \"Parent folder path\"  # Specify the parent folder path\n",
        "    directories = [(os.path.join(path, d), parent) for path, dirs, files in os.walk(parent) for d in dirs]\n",
        "\n",
        "    # Use all available CPUs (one cpu per csv)\n",
        "    with Pool(cpu_count()) as pool:\n",
        "        pool.map(process_directory, directories)\n"
      ],
      "metadata": {
        "id": "fu1RlfhayPYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Similarity check"
      ],
      "metadata": {
        "id": "NaykhZl6zZup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "path = \"-------\"\n",
        "output_path = \"---\"     #deduplication ke bad jidher save kr rhe apan\n",
        "log_file_path = \"------\"        # sare duplicates idher store kr lo\n",
        "\n",
        "files = list(os.listdir(path))\n",
        "files.sort()\n",
        "print(\"Number of files:\", len(files), files)\n",
        "files = [f\"{path}/{x}\" for x in files]\n",
        "\n",
        "with open(log_file_path, 'a') as log_file:\n",
        "    for file in tqdm(files):\n",
        "        print(file)\n",
        "        df = pd.read_csv(file)\n",
        "\n",
        "        # Identify duplicates\n",
        "        duplicates = df[df.duplicated(subset='hash', keep=False)]\n",
        "\n",
        "        if not duplicates.empty:\n",
        "            log_file.write(f\"Duplicates in {file}:\\n\")\n",
        "            for index, row in duplicates.iterrows():\n",
        "                log_file.write(f\"\\tFile: {row['file']} Hash: {row['hash']}\\n\")\n",
        "            log_file.write(\"\\n\")\n",
        "\n",
        "        # Remove duplicates\n",
        "        df = df.drop_duplicates(subset='hash', keep=\"first\")\n",
        "\n",
        "        s = file.split(\"/\")[-1]\n",
        "        print(s)\n",
        "        df.to_csv(f\"{output_path}/{s}\", index=False)\n",
        "        print(\"Deduplication done for\", file)\n",
        "\n",
        "print(f\"Duplicate log has been saved to {log_file_path}\")"
      ],
      "metadata": {
        "id": "xYsn1TQYySPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Code to transfer all deduplicated files in another path"
      ],
      "metadata": {
        "id": "A917ojpFzdd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import concurrent.futures\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define the directories\n",
        "csv_folder = \"-----\"        #deduplication ke bad jidher save kr rhe apan\n",
        "target_dir = \"---\"       # Directory where the files will be copied to\n",
        "original_dir = \"----------\"       #Directory containing the original files that need to be copied based on the paths extracted from the CSVs.\n",
        "\n",
        "def copy_file(file):\n",
        "    \"\"\"Copy a file to the target directory while preserving the relative path.\"\"\"\n",
        "    try:\n",
        "        relative_path = os.path.relpath(file, original_dir)\n",
        "        target_path = os.path.join(target_dir, relative_path)\n",
        "        os.makedirs(os.path.dirname(target_path), exist_ok=True)\n",
        "        shutil.copy(file, target_path)\n",
        "        return f\"Copied {file} to {target_path}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error copying {file} to {target_path}: {str(e)}\"\n",
        "\n",
        "def process_csv(csv_file):\n",
        "    \"\"\"Process a single CSV file.\"\"\"\n",
        "    csv_path = os.path.join(csv_folder, csv_file)\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        if 'file' in df.columns:\n",
        "            file_paths = df['file'].tolist()\n",
        "            return file_paths\n",
        "        else:\n",
        "            print(f\"CSV {csv_file} does not contain 'file' column.\")\n",
        "            return []\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {csv_file}: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def main():\n",
        "    all_files = []\n",
        "\n",
        "    # Gather all file paths from the CSV files using multiprocessing\n",
        "    csv_files = [csv_file for csv_file in os.listdir(csv_folder) if csv_file.endswith('.csv')]\n",
        "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
        "        results = list(tqdm(executor.map(process_csv, csv_files), total=len(csv_files), desc=\"Loading CSVs\"))\n",
        "\n",
        "    for file_list in results:\n",
        "        all_files.extend(file_list)\n",
        "\n",
        "    # Use ThreadPoolExecutor to copy files in parallel\n",
        "    futures = {}\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        for file_path in tqdm(all_files, desc=\"Submitting copy tasks\"):\n",
        "            if os.path.exists(os.path.join(original_dir, file_path)):\n",
        "                future = executor.submit(copy_file, os.path.join(original_dir, file_path))\n",
        "                futures[future] = file_path\n",
        "\n",
        "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Copying files\"):\n",
        "            print(future.result())\n",
        "\n",
        "# Run the script\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "Y4J5MVapyU2l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}