{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4iQhY8MXQDR"
      },
      "source": [
        "# Hi y'all, this is Team-5 and here we've documented the entire deduplication methods and pipelines utilised in our assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miHblrp5X9eG"
      },
      "source": [
        "## The primary common step of reading the input folders and writing the output folders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3eWn1ABYQkN"
      },
      "source": [
        "We have one parent folder which in turn stores a number of folders corresponding to each unique source. These folders in turn contain a list of .txt files. Through our deduplication methodoligies, we read through all the .txt files accross all the folders and remove the duplicate .txt files which have similarity scores above a pre-set threshold."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni8-ysx1ZvTR"
      },
      "source": [
        "The following code helps in reading and storing all the text files across all the input folders; and writing the unique text files into the new output folders in the same manner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "jZJH4oUaXMbY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datasketch import MinHash, MinHashLSH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMZXRRA6ePyq"
      },
      "source": [
        "First of all, we'll load all the text files from our folder structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import chardet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "S1FZhqPsaMyK"
      },
      "outputs": [],
      "source": [
        "def detect_file_encoding(file_path):\n",
        "    # Open the file in binary mode for encoding detection\n",
        "    with open(file_path, 'rb') as file:\n",
        "        raw_data = file.read()\n",
        "        result = chardet.detect(raw_data)\n",
        "        encoding = result['encoding']\n",
        "        confidence = result['confidence']\n",
        "        return encoding, confidence\n",
        "\n",
        "def load_documents_from_folders(root_folder):\n",
        "    documents = []\n",
        "    file_paths = []\n",
        "\n",
        "    # Recursively walking through all subfolders and files\n",
        "    for dirpath, _, filenames in os.walk(root_folder):\n",
        "        for filename in filenames:\n",
        "            if filename.endswith(\".txt\"):  # Process only .txt files\n",
        "                file_path = os.path.join(dirpath, filename)\n",
        "                file_paths.append(file_path)\n",
        "\n",
        "                # Detect encoding of the file\n",
        "                encoding, confidence = detect_file_encoding(file_path)\n",
        "                print(f\"Detected encoding for {file_path}: {encoding} (confidence: {confidence})\")\n",
        "\n",
        "                try:\n",
        "                    # Use the detected encoding to read the file\n",
        "                    with open(file_path, 'r', encoding=encoding) as file:\n",
        "                        documents.append(file.read())\n",
        "                except Exception as e:\n",
        "                    print(f\"Error reading file {file_path}: {e}\")\n",
        "\n",
        "    return documents, file_paths\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKpvuBZYfFgl"
      },
      "source": [
        "Here, we're writing the unique documents back to the output folder, while preserving our document structure as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "urxbxGaVaPUI"
      },
      "outputs": [],
      "source": [
        "def save_unique_documents(unique_docs, original_file_paths, output_folder):\n",
        "    for doc_id in unique_docs:\n",
        "        original_file_path = original_file_paths[int(doc_id.split('_')[1])]\n",
        "\n",
        "        #recreating the original folder structure in the output folder\n",
        "        relative_path = os.path.relpath(original_file_path, start=input_folder)\n",
        "        output_file_path = os.path.join(output_folder, relative_path)\n",
        "        os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n",
        "\n",
        "        #copying the content to the new file\n",
        "        with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
        "            out_file.write(documents[int(doc_id.split('_')[1])])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJ2rIF-JfiIU"
      },
      "source": [
        "Storing the paths to our input and output parent folder, also loading the documents and file paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "tcJtfy2CaR71"
      },
      "outputs": [],
      "source": [
        "# Path to your input folder and output folder\n",
        "input_folder = r\"C:\\Users\\jiyad\\Downloads\\parentfolder\"\n",
        "output_folder = r\"C:\\Users\\jiyad\\OneDrive\\Desktop\\IITGN\\NLP\\deduplicated_dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: charset-normalizer in c:\\users\\jiyad\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.3.2)Note: you may need to restart the kernel to use updated packages.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pip install charset-normalizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Zx-b7VjmaVCF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\parentfolder\\f1\\22.txt: utf-8 (confidence: 0.99)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\parentfolder\\f1\\27_copy.txt: utf-8 (confidence: 0.99)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\parentfolder\\f1\\32.txt: utf-8 (confidence: 0.99)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\parentfolder\\f2\\31.txt: utf-8 (confidence: 0.99)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\parentfolder\\f2\\36.txt: utf-8 (confidence: 0.99)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\parentfolder\\f2\\37.txt: utf-8 (confidence: 0.99)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\parentfolder\\f3\\22+plus some random.txt: utf-8 (confidence: 0.99)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\parentfolder\\f3\\34.txt: utf-8 (confidence: 0.99)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\parentfolder\\f3\\35.txt: utf-8 (confidence: 0.99)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\parentfolder\\f4\\30.txt: utf-8 (confidence: 0.99)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\parentfolder\\f4\\31_halved.txt: utf-8 (confidence: 0.99)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\parentfolder\\f4\\33.txt: utf-8 (confidence: 0.99)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\parentfolder\\f4\\35_copy.txt: utf-8 (confidence: 0.99)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\parentfolder\\f4\\38.txt: utf-8 (confidence: 0.99)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\parentfolder\\f5\\29.txt: utf-8 (confidence: 0.99)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\parentfolder\\f5\\37_copy.txt: utf-8 (confidence: 0.99)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\parentfolder\\f6\\23.txt: utf-8 (confidence: 0.99)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\parentfolder\\f6\\24.txt: utf-8 (confidence: 0.99)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\parentfolder\\f6\\25.txt: utf-8 (confidence: 0.99)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\parentfolder\\f6\\27.txt: utf-8 (confidence: 0.99)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\parentfolder\\f6\\36+34.txt: utf-8 (confidence: 0.99)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\__MACOSX\\parentfolder\\f1\\._22.txt: ISO-8859-1 (confidence: 0.6748561151079137)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\__MACOSX\\parentfolder\\f1\\._27_copy.txt: ISO-8859-1 (confidence: 0.6748561151079137)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\__MACOSX\\parentfolder\\f1\\._32.txt: ISO-8859-1 (confidence: 0.6841017964071856)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\__MACOSX\\parentfolder\\f2\\._31.txt: ISO-8859-1 (confidence: 0.6748561151079137)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\__MACOSX\\parentfolder\\f2\\._36.txt: ISO-8859-1 (confidence: 0.6748561151079137)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\__MACOSX\\parentfolder\\f2\\._37.txt: ISO-8859-1 (confidence: 0.6748561151079137)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\__MACOSX\\parentfolder\\f3\\._22+plus some random.txt: MacRoman (confidence: 0.4786582278481013)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\__MACOSX\\parentfolder\\f3\\._34.txt: ISO-8859-1 (confidence: 0.5923053892215568)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\__MACOSX\\parentfolder\\f3\\._35.txt: ISO-8859-1 (confidence: 0.6748561151079137)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\__MACOSX\\parentfolder\\f4\\._30.txt: ISO-8859-1 (confidence: 0.6748561151079137)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\__MACOSX\\parentfolder\\f4\\._31_halved.txt: MacRoman (confidence: 0.4786582278481013)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\__MACOSX\\parentfolder\\f4\\._33.txt: ISO-8859-1 (confidence: 0.6748561151079137)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\__MACOSX\\parentfolder\\f4\\._35_copy.txt: ISO-8859-1 (confidence: 0.6748561151079137)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\__MACOSX\\parentfolder\\f4\\._38.txt: ISO-8859-1 (confidence: 0.6748561151079137)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\__MACOSX\\parentfolder\\f5\\._29.txt: ISO-8859-1 (confidence: 0.6748561151079137)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\__MACOSX\\parentfolder\\f5\\._37_copy.txt: ISO-8859-1 (confidence: 0.6748561151079137)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\__MACOSX\\parentfolder\\f6\\._23.txt: ISO-8859-1 (confidence: 0.6748561151079137)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\__MACOSX\\parentfolder\\f6\\._24.txt: ISO-8859-1 (confidence: 0.6748561151079137)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\__MACOSX\\parentfolder\\f6\\._25.txt: ISO-8859-1 (confidence: 0.6748561151079137)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\__MACOSX\\parentfolder\\f6\\._27.txt: ISO-8859-1 (confidence: 0.6748561151079137)\n",
            "Detected encoding for C:\\Users\\jiyad\\Downloads\\parentfolder\\__MACOSX\\parentfolder\\f6\\._36+34.txt: Windows-1254 (confidence: 0.3927159788277466)\n",
            "Error reading file C:\\Users\\jiyad\\Downloads\\parentfolder\\__MACOSX\\parentfolder\\f6\\._36+34.txt: 'charmap' codec can't decode byte 0x8e in position 383: character maps to <undefined>\n"
          ]
        }
      ],
      "source": [
        "documents, file_paths = load_documents_from_folders(input_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gd0EGmo0ahOt"
      },
      "source": [
        "Now, we've successfully managed to input all the .txt. files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wysBhuiLapMY"
      },
      "source": [
        "## Deduplication Pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eE4nUslavS6"
      },
      "source": [
        "### Technique 1 - MinHash+LSH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HjPYct2a5ww"
      },
      "source": [
        "First of all we do a basic text normalization to improve the deduplication by lowercasing and removing extra spaces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "30eczEEJak6a"
      },
      "outputs": [],
      "source": [
        "def normalize_text(text):\n",
        "    return ' '.join(text.lower().split())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AkywrQVbOPV"
      },
      "source": [
        "Now, the major step comes in of creating a MinHash object and an LSH model to group similar documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "SvmSzuVQbOpX",
        "outputId": "779a26c9-6ad0-4b9c-921a-8941a32087b4"
      },
      "outputs": [],
      "source": [
        "def compute_minhash(doc):\n",
        "    m = MinHash(num_perm=128)\n",
        "    normalized_doc = normalize_text(doc)  #Applying normalization\n",
        "    for word in normalized_doc.split():\n",
        "        m.update(word.encode('utf8'))\n",
        "    return m\n",
        "\n",
        "threshold = 0.85  #Setting similarity threshold for near-duplicate detection\n",
        "lsh = MinHashLSH(threshold=threshold, num_perm=128)\n",
        "\n",
        "#Inserting MinHashes into the LSH\n",
        "minhashes = {}\n",
        "for idx, doc in enumerate(documents):\n",
        "    m = compute_minhash(doc)\n",
        "    lsh.insert(f'doc_{idx}', m)\n",
        "    minhashes[idx] = m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzdQpsiCdRym"
      },
      "source": [
        "The next step is pretty straightforward, we'll keep a track of unique and duplicates documents and ensure that the copy of the parent folder we're creating as an output only stores the unique ones (the first occurence among the cases of duplications)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "T2r41F5Wc5sw"
      },
      "outputs": [],
      "source": [
        "unique_docs = set()\n",
        "duplicates = set()\n",
        "\n",
        "for idx, doc in enumerate(documents):\n",
        "    m = minhashes[idx]\n",
        "\n",
        "    if f'doc_{idx}' not in unique_docs:\n",
        "        #Querying similar documents from LSH\n",
        "        similar_docs = lsh.query(m)\n",
        "        for sim_doc in similar_docs:\n",
        "            if sim_doc != f'doc_{idx}':\n",
        "                duplicates.add(sim_doc)\n",
        "        unique_docs.add(f'doc_{idx}')\n",
        "\n",
        "#Filtering out duplicates\n",
        "filtered_docs = [f'doc_{idx}' for idx in range(len(documents)) if f'doc_{idx}' not in duplicates]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sp5165f7d9Cp"
      },
      "source": [
        "Saving the unique documents to the output folder, maintaining folder structure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "9BBlVy1Hd7ML"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deduplicated documents saved to C:\\Users\\jiyad\\OneDrive\\Desktop\\IITGN\\NLP\\deduplicated_dataset\n"
          ]
        }
      ],
      "source": [
        "save_unique_documents(filtered_docs, file_paths, output_folder)\n",
        "\n",
        "print(f\"Deduplicated documents saved to {output_folder}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blrOhdwreIRv"
      },
      "source": [
        "Voila!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSiQsbFKh8P3"
      },
      "source": [
        "### Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zj0cUQJ1h-LD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
