{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Login to huggingface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:50:30.888616Z",
     "iopub.status.busy": "2024-11-22T21:50:30.888259Z",
     "iopub.status.idle": "2024-11-22T21:50:31.247996Z",
     "shell.execute_reply": "2024-11-22T21:50:31.247025Z",
     "shell.execute_reply.started": "2024-11-22T21:50:30.888573Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# login to huggingface snippet\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required installation and imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:50:59.165874Z",
     "iopub.status.busy": "2024-11-22T21:50:59.164978Z",
     "iopub.status.idle": "2024-11-22T21:51:07.937495Z",
     "shell.execute_reply": "2024-11-22T21:51:07.936254Z",
     "shell.execute_reply.started": "2024-11-22T21:50:59.165819Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install rouge_score pycocoevalcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:51:13.210502Z",
     "iopub.status.busy": "2024-11-22T21:51:13.210109Z",
     "iopub.status.idle": "2024-11-22T21:51:22.077842Z",
     "shell.execute_reply": "2024-11-22T21:51:22.076702Z",
     "shell.execute_reply.started": "2024-11-22T21:51:13.210468Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:51:27.842345Z",
     "iopub.status.busy": "2024-11-22T21:51:27.841951Z",
     "iopub.status.idle": "2024-11-22T21:51:36.628655Z",
     "shell.execute_reply": "2024-11-22T21:51:36.627445Z",
     "shell.execute_reply.started": "2024-11-22T21:51:27.842309Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:51:54.819227Z",
     "iopub.status.busy": "2024-11-22T21:51:54.818823Z",
     "iopub.status.idle": "2024-11-22T21:52:02.198924Z",
     "shell.execute_reply": "2024-11-22T21:52:02.198032Z",
     "shell.execute_reply.started": "2024-11-22T21:51:54.819191Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import datasets\n",
    "from torch import nn\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoModel,\n",
    "                          AutoModelForCausalLM, \n",
    "                          AutoTokenizer,\n",
    "                          AutoModelForQuestionAnswering,\n",
    "                          Trainer, TrainingArguments,\n",
    "                          DataCollatorWithPadding\n",
    "                          )\n",
    "import evaluate\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "print(\"imports done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:52:10.523759Z",
     "iopub.status.busy": "2024-11-22T21:52:10.522670Z",
     "iopub.status.idle": "2024-11-22T21:52:10.551534Z",
     "shell.execute_reply": "2024-11-22T21:52:10.550621Z",
     "shell.execute_reply.started": "2024-11-22T21:52:10.523719Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model - both the base model and the model with question-answering head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:52:14.712334Z",
     "iopub.status.busy": "2024-11-22T21:52:14.711632Z",
     "iopub.status.idle": "2024-11-22T21:52:31.044558Z",
     "shell.execute_reply": "2024-11-22T21:52:31.043714Z",
     "shell.execute_reply.started": "2024-11-22T21:52:14.712289Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# setting the padding token manually:\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"tokenizer loaded\")\n",
    "\n",
    "# model without question-answering layer:\n",
    "base_model = AutoModel.from_pretrained(model_name)\n",
    "print(\"base model loaded\")\n",
    "\n",
    "# model with question-answering layer:\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "print(\"model with question answering layer loaded\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting the model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:53:03.031802Z",
     "iopub.status.busy": "2024-11-22T21:53:03.030934Z",
     "iopub.status.idle": "2024-11-22T21:53:03.039011Z",
     "shell.execute_reply": "2024-11-22T21:53:03.037780Z",
     "shell.execute_reply.started": "2024-11-22T21:53:03.031751Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# function to count model parameters:\n",
    "def print_model_parameters_tabular(model):\n",
    "    parameters = []\n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "\n",
    "    # Collect model parameter details\n",
    "    for name, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        total_params += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "        parameters.append([name, list(param.shape), param.requires_grad, f\"{num_params:,}\"])\n",
    "\n",
    "    # Print parameter details in a more compact, line-by-line format\n",
    "    print(f\"Model Parameters for {type(model).__name__}:\\n\")\n",
    "    for param in parameters:\n",
    "        name, shape, requires_grad, num_elements = param\n",
    "        print(f\"Parameter Name: {name}\")\n",
    "        print(f\"  Shape: {shape}\")\n",
    "        print(f\"  Requires Grad: {requires_grad}\")\n",
    "        print(f\"  Total Elements: {num_elements}\")\n",
    "        print(\"-\" * 50)  # Separator line for clarity\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nTotal Parameters: {total_params:,}\")\n",
    "    print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "    print(f\"Non-Trainable Parameters: {total_params - trainable_params:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without question-answering layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:53:07.652458Z",
     "iopub.status.busy": "2024-11-22T21:53:07.651818Z",
     "iopub.status.idle": "2024-11-22T21:53:07.662438Z",
     "shell.execute_reply": "2024-11-22T21:53:07.661510Z",
     "shell.execute_reply.started": "2024-11-22T21:53:07.652421Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# printing parameters of base model:\n",
    "print_model_parameters_tabular(base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With question-answering layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:53:18.106628Z",
     "iopub.status.busy": "2024-11-22T21:53:18.105761Z",
     "iopub.status.idle": "2024-11-22T21:53:18.117229Z",
     "shell.execute_reply": "2024-11-22T21:53:18.116188Z",
     "shell.execute_reply.started": "2024-11-22T21:53:18.106585Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print_model_parameters_tabular(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As can be seen, the addition of a question-answering head to make the model suitable for a question-answering task adds 4098 extra parameters to the base model. During fine-tuning, we will only be training these parameters and not change the parameters of the base model (due to memory and computation constraints - and also, the relevant parameters for question-answering are only those added in the final layer).\n",
    "\n",
    "### We will also check the number of parameters after fine-tuning, to show that fine-tuning by itself doesn't alter the number of parameters, but adding an extra layer does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:53:36.074575Z",
     "iopub.status.busy": "2024-11-22T21:53:36.073902Z",
     "iopub.status.idle": "2024-11-22T21:53:37.727325Z",
     "shell.execute_reply": "2024-11-22T21:53:37.726437Z",
     "shell.execute_reply.started": "2024-11-22T21:53:36.074537Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"rajpurkar/squad_v2\", split=\"train\")\n",
    "print(dataset)\n",
    "\n",
    "# splitting on the default 'train' split as asked, with a train-test ratio of 80:20 and random_state = 1 \n",
    "split_dataset = dataset.train_test_split(test_size=0.2, seed=1)\n",
    "print(split_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:53:41.900396Z",
     "iopub.status.busy": "2024-11-22T21:53:41.899623Z",
     "iopub.status.idle": "2024-11-22T21:53:41.905477Z",
     "shell.execute_reply": "2024-11-22T21:53:41.904450Z",
     "shell.execute_reply.started": "2024-11-22T21:53:41.900359Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# training/testing sample:\n",
    "print(split_dataset['train'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:53:57.265959Z",
     "iopub.status.busy": "2024-11-22T21:53:57.265591Z",
     "iopub.status.idle": "2024-11-22T21:53:57.275709Z",
     "shell.execute_reply": "2024-11-22T21:53:57.274670Z",
     "shell.execute_reply.started": "2024-11-22T21:53:57.265926Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# function to tokenize the data (takes the sample printed above as input)\n",
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=128,\n",
    "        truncation=\"only_second\",\n",
    "        stride=64,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    inputs[\"example_id\"] = [examples[\"id\"][i] for i in sample_map]\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while idx < len(sequence_ids) and sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while idx < len(sequence_ids) and sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:54:02.540474Z",
     "iopub.status.busy": "2024-11-22T21:54:02.540114Z",
     "iopub.status.idle": "2024-11-22T21:54:12.997524Z",
     "shell.execute_reply": "2024-11-22T21:54:12.996611Z",
     "shell.execute_reply.started": "2024-11-22T21:54:02.540444Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# calling the tokenizing function on training + testing samples:\n",
    "tokenized_train_data = split_dataset['train'].map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=split_dataset[\"train\"].column_names, # to make the format suitable for the model\n",
    ")\n",
    "print(\"train data tokenized\")\n",
    "tokenized_test_data = split_dataset['test'].map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=split_dataset[\"test\"].column_names,\n",
    ")\n",
    "print(\"test data tokenized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying tokenized data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:54:22.383992Z",
     "iopub.status.busy": "2024-11-22T21:54:22.383297Z",
     "iopub.status.idle": "2024-11-22T21:54:22.389757Z",
     "shell.execute_reply": "2024-11-22T21:54:22.388615Z",
     "shell.execute_reply.started": "2024-11-22T21:54:22.383956Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(tokenized_train_data[0])\n",
    "print(tokenized_test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:54:26.627715Z",
     "iopub.status.busy": "2024-11-22T21:54:26.626864Z",
     "iopub.status.idle": "2024-11-22T21:54:26.631626Z",
     "shell.execute_reply": "2024-11-22T21:54:26.630650Z",
     "shell.execute_reply.started": "2024-11-22T21:54:26.627681Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# data collator instance:\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining training arguments and creating trainer instance (before fine-tuning for pre-fine-tuning evaluation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:55:07.627440Z",
     "iopub.status.busy": "2024-11-22T21:55:07.627014Z",
     "iopub.status.idle": "2024-11-22T21:55:09.300991Z",
     "shell.execute_reply": "2024-11-22T21:55:09.300019Z",
     "shell.execute_reply.started": "2024-11-22T21:55:07.627408Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:55:18.459638Z",
     "iopub.status.busy": "2024-11-22T21:55:18.459277Z",
     "iopub.status.idle": "2024-11-22T21:55:18.498403Z",
     "shell.execute_reply": "2024-11-22T21:55:18.497711Z",
     "shell.execute_reply.started": "2024-11-22T21:55:18.459608Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# defining training arguments:\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",     \n",
    "    per_device_eval_batch_size=8, # due to hardware constraints\n",
    "    do_eval=False,                  \n",
    "    logging_dir='./logs',         \n",
    "    report_to=\"none\", \n",
    ")\n",
    "\n",
    "# trainer instance:\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,      \n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to compute various evaluation metrics and compare pre- and post-fine-tuning performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:55:28.669342Z",
     "iopub.status.busy": "2024-11-22T21:55:28.668454Z",
     "iopub.status.idle": "2024-11-22T21:55:28.674835Z",
     "shell.execute_reply": "2024-11-22T21:55:28.673814Z",
     "shell.execute_reply.started": "2024-11-22T21:55:28.669297Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compare_predictions(test_data, squad_dataset_test, start_idx, end_idx, tokenizer):\n",
    "\n",
    "    original_answers_map = {\n",
    "        example[\"id\"]: example[\"answers\"][\"text\"][0] for example in squad_dataset_test\n",
    "    }\n",
    "    comparisons = [\n",
    "        (\n",
    "            original_answers_map[example_id],  # Original answer\n",
    "            tokenizer.decode(\n",
    "                test_data[\"input_ids\"][i][start_idx[i]:end_idx[i] + 1]\n",
    "            ).strip() if start_idx[i] != 0 and end_idx[i] != 0 else \"\"\n",
    "        )\n",
    "        for i, example_id in enumerate(test_data[\"example_id\"])\n",
    "    ]\n",
    "\n",
    "    return comparisons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:55:31.656589Z",
     "iopub.status.busy": "2024-11-22T21:55:31.656240Z",
     "iopub.status.idle": "2024-11-22T21:55:31.660802Z",
     "shell.execute_reply": "2024-11-22T21:55:31.659818Z",
     "shell.execute_reply.started": "2024-11-22T21:55:31.656559Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_average_meteor_scores(references, predictions):\n",
    "    total = 0\n",
    "    count = len(predictions)\n",
    "\n",
    "    for ref, pred in zip(references, predictions):\n",
    "        ref = word_tokenize(ref)\n",
    "        pred = word_tokenize(pred)\n",
    "        score_details = meteor_score([ref], pred)\n",
    "        total += score_details\n",
    "\n",
    "    avg = total / count\n",
    "\n",
    "    return {\n",
    "        \"score\": avg,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:58:10.405442Z",
     "iopub.status.busy": "2024-11-22T21:58:10.404772Z",
     "iopub.status.idle": "2024-11-22T21:58:10.414495Z",
     "shell.execute_reply": "2024-11-22T21:58:10.413571Z",
     "shell.execute_reply.started": "2024-11-22T21:58:10.405407Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_function(test_data, tokenizer, squad_dataset_test, trainer):\n",
    "    predictions, _, _ = trainer.predict(test_data)\n",
    "    start_logits, end_logits = predictions\n",
    "    start_idx = np.argmax(start_logits, axis=1)\n",
    "    end_idx = np.argmax(end_logits, axis=1)\n",
    "\n",
    "    comparisons = compare_predictions(test_data, squad_dataset_test, start_idx, end_idx, tokenizer)\n",
    "    original_answers = [original for original, _ in comparisons]\n",
    "    predicted_answers = [predicted for _, predicted in comparisons]\n",
    "    print(\"hey\")\n",
    "    squad_predictions = [\n",
    "        {\n",
    "            \"id\": str(i),\n",
    "            \"prediction_text\": pred,\n",
    "            \"no_answer_probability\": 1.0 if len(pred.strip()) == 0 else 0.0,\n",
    "        }\n",
    "        for i, pred in enumerate(predicted_answers)\n",
    "    ]\n",
    "\n",
    "    squad_references = [\n",
    "        {\"id\": str(i), \"answers\": {\"text\": [orig], \"answer_start\": []}}\n",
    "        for i, orig in enumerate(original_answers)\n",
    "    ]\n",
    "    squad_metric = evaluate.load(\"squad_v2\")\n",
    "    bleu_metric = evaluate.load(\"bleu\")\n",
    "    rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "    squad_results = squad_metric.compute(\n",
    "        predictions=squad_predictions,\n",
    "        references=squad_references\n",
    "    )\n",
    "    \n",
    "    bleu_results = bleu_metric.compute(\n",
    "        predictions=predicted_answers,\n",
    "        references=[[orig] for orig in original_answers]\n",
    "    )\n",
    "    \n",
    "    rouge_results = rouge_metric.compute(\n",
    "        predictions=predicted_answers,\n",
    "        references=original_answers\n",
    "    )\n",
    "    \n",
    "    meteor_results = compute_average_meteor_scores(original_answers, predicted_answers)\n",
    "\n",
    "    # final output - returning all metrics:\n",
    "    results = {\n",
    "        \"squad_v2\": squad_results,\n",
    "        \"bleu\": bleu_results[\"bleu\"],\n",
    "        \"rouge-2\": rouge_results[\"rouge2\"],\n",
    "        \"rouge-L\": rouge_results[\"rougeL\"],\n",
    "        \"rouge-1\": rouge_results[\"rouge1\"],\n",
    "        \"meteor\": meteor_results,\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:58:14.911882Z",
     "iopub.status.busy": "2024-11-22T21:58:14.911449Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "results_before = evaluate_function(tokenized_test_data, tokenizer, split_dataset[\"test\"], trainer)\n",
    "print(results_before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freezing all model parameters except the last question-answering layer added to the base model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for param in model.transformer.parameters():\n",
    "    param.requires_grad = False\n",
    "# verify if the top layer is still trainable\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)# Define the training arguments\n",
    "\n",
    "# training arguments:\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./post_finetuning_results\",            \n",
    "    evaluation_strategy=\"no\",                    \n",
    "    learning_rate=2e-5,                          \n",
    "    per_device_train_batch_size=8,  # set to a smaller number due to memory and compute constraints\n",
    "    per_device_eval_batch_size=8,                \n",
    "    num_train_epochs=3,                          \n",
    "    weight_decay=0.01,                           \n",
    "    logging_dir=\"./logs\",                        \n",
    "    report_to=\"none\",                      \n",
    "    save_strategy=\"epoch\",                      \n",
    "    fp16=True,                                  \n",
    "    logging_steps=10,                           \n",
    "    lr_scheduler_type=\"linear\",                 \n",
    "    load_best_model_at_end=False,                \n",
    ")\n",
    "\n",
    "# instantiating trainer for fine-tuning\n",
    "trainer = Trainer(\n",
    "    model=model,       \n",
    "    args=training_args,               \n",
    "    train_dataset=tokenized_train_data, \n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results_after = evaluate_function(tokenized_test_data, tokenizer, split_dataset[\"test\"], trainer)\n",
    "print(results_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def create_single_table(results_before, results_after):\n",
    "    def format_value(value):\n",
    "        \"\"\"Helper function to format values consistently.\"\"\"\n",
    "        if isinstance(value, float):\n",
    "            return f\"{value:.4f}\"\n",
    "        return value if value is not None else \"N/A\"\n",
    "\n",
    "    # Collect all keys\n",
    "    all_keys = set(results_before.keys()).union(results_after.keys())\n",
    "\n",
    "    # Prepare data for the table\n",
    "    table_data = []\n",
    "    for key in sorted(all_keys):\n",
    "        if isinstance(results_before.get(key), dict) or isinstance(results_after.get(key), dict):\n",
    "            # Handle nested dictionaries\n",
    "            sub_keys = set(results_before.get(key, {}).keys()).union(results_after.get(key, {}).keys())\n",
    "            for sub_key in sorted(sub_keys):\n",
    "                before_value = format_value(results_before.get(key, {}).get(sub_key))\n",
    "                after_value = format_value(results_after.get(key, {}).get(sub_key))\n",
    "                table_data.append([key, sub_key, before_value, after_value])\n",
    "        else:\n",
    "            # Handle scalar values\n",
    "            before_value = format_value(results_before.get(key))\n",
    "            after_value = format_value(results_after.get(key))\n",
    "            table_data.append([key, \"-\", before_value, after_value])\n",
    "\n",
    "    # Create the figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(10, len(table_data) * 0.5))\n",
    "    ax.axis(\"tight\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    # Add a title\n",
    "    title = \"Comparison of Results Before and After Fine-Tuning\"\n",
    "    fig.suptitle(title, fontsize=14, fontweight=\"bold\", y=0.98)\n",
    "\n",
    "    # Create the table\n",
    "    column_labels = [\"Metric\", \"Sub-Metric\", \"Before Fine-Tuning\", \"After Fine-Tuning\"]\n",
    "    table = ax.table(\n",
    "        cellText=table_data,\n",
    "        colLabels=column_labels,\n",
    "        cellLoc=\"center\",\n",
    "        loc=\"center\",\n",
    "    )\n",
    "\n",
    "    # Style the table\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.auto_set_column_width(col=list(range(len(column_labels))))\n",
    "\n",
    "    # Display the table\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "repo_name = \"jiya14desai/Llama-3.2-1B_fine-tuned_on_questionanswering\"\n",
    "model.push_to_hub(repo_name)\n",
    "print(f\"Model uploaded to Hugging Face Hub under repository: {repo_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "\n",
    "pretrained_params = count_parameters(model)\n",
    "finetuned_params = count_parameters(trainer.model)\n",
    "\n",
    "print(f\"Number of parameters in the pre-trained model: {pretrained_params}\")\n",
    "print(f\"Number of parameters in the fine-tuned model: {finetuned_params}\")\n",
    "if pretrained_params == finetuned_params:\n",
    "    print(\"The number of parameters remains the same after fine-tuning.\")\n",
    "else:\n",
    "    print(\"The number of parameters differs after fine-tuning.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
