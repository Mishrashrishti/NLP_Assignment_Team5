{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Login to huggingface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:50:30.888616Z",
     "iopub.status.busy": "2024-11-22T21:50:30.888259Z",
     "iopub.status.idle": "2024-11-22T21:50:31.247996Z",
     "shell.execute_reply": "2024-11-22T21:50:31.247025Z",
     "shell.execute_reply.started": "2024-11-22T21:50:30.888573Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# login to huggingface snippet\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required installation and imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:50:59.165874Z",
     "iopub.status.busy": "2024-11-22T21:50:59.164978Z",
     "iopub.status.idle": "2024-11-22T21:51:07.937495Z",
     "shell.execute_reply": "2024-11-22T21:51:07.936254Z",
     "shell.execute_reply.started": "2024-11-22T21:50:59.165819Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge_score in /opt/conda/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: pycocoevalcap in /opt/conda/lib/python3.10/site-packages (1.2)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from pycocoevalcap) (2.0.8)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools>=2.0.2->pycocoevalcap) (3.7.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (2.9.0.post0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rouge_score pycocoevalcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:51:13.210502Z",
     "iopub.status.busy": "2024-11-22T21:51:13.210109Z",
     "iopub.status.idle": "2024-11-22T21:51:22.077842Z",
     "shell.execute_reply": "2024-11-22T21:51:22.076702Z",
     "shell.execute_reply.started": "2024-11-22T21:51:13.210468Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.25.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:51:27.842345Z",
     "iopub.status.busy": "2024-11-22T21:51:27.841951Z",
     "iopub.status.idle": "2024-11-22T21:51:36.628655Z",
     "shell.execute_reply": "2024-11-22T21:51:36.627445Z",
     "shell.execute_reply.started": "2024-11-22T21:51:27.842309Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:51:54.819227Z",
     "iopub.status.busy": "2024-11-22T21:51:54.818823Z",
     "iopub.status.idle": "2024-11-22T21:52:02.198924Z",
     "shell.execute_reply": "2024-11-22T21:52:02.198032Z",
     "shell.execute_reply.started": "2024-11-22T21:51:54.819191Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import datasets\n",
    "from torch import nn\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoModel,\n",
    "                          AutoModelForCausalLM, \n",
    "                          AutoTokenizer,\n",
    "                          AutoModelForQuestionAnswering,\n",
    "                          Trainer, TrainingArguments,\n",
    "                          DataCollatorWithPadding\n",
    "                          )\n",
    "import evaluate\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "print(\"imports done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:52:10.523759Z",
     "iopub.status.busy": "2024-11-22T21:52:10.522670Z",
     "iopub.status.idle": "2024-11-22T21:52:10.551534Z",
     "shell.execute_reply": "2024-11-22T21:52:10.550621Z",
     "shell.execute_reply.started": "2024-11-22T21:52:10.523719Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model - both the base model and the model with question-answering head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:52:14.712334Z",
     "iopub.status.busy": "2024-11-22T21:52:14.711632Z",
     "iopub.status.idle": "2024-11-22T21:52:31.044558Z",
     "shell.execute_reply": "2024-11-22T21:52:31.043714Z",
     "shell.execute_reply.started": "2024-11-22T21:52:14.712289Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer loaded\n",
      "base model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForQuestionAnswering were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['embed_tokens.weight', 'layers.0.input_layernorm.weight', 'layers.0.mlp.down_proj.weight', 'layers.0.mlp.gate_proj.weight', 'layers.0.mlp.up_proj.weight', 'layers.0.post_attention_layernorm.weight', 'layers.0.self_attn.k_proj.weight', 'layers.0.self_attn.o_proj.weight', 'layers.0.self_attn.q_proj.weight', 'layers.0.self_attn.v_proj.weight', 'layers.1.input_layernorm.weight', 'layers.1.mlp.down_proj.weight', 'layers.1.mlp.gate_proj.weight', 'layers.1.mlp.up_proj.weight', 'layers.1.post_attention_layernorm.weight', 'layers.1.self_attn.k_proj.weight', 'layers.1.self_attn.o_proj.weight', 'layers.1.self_attn.q_proj.weight', 'layers.1.self_attn.v_proj.weight', 'layers.10.input_layernorm.weight', 'layers.10.mlp.down_proj.weight', 'layers.10.mlp.gate_proj.weight', 'layers.10.mlp.up_proj.weight', 'layers.10.post_attention_layernorm.weight', 'layers.10.self_attn.k_proj.weight', 'layers.10.self_attn.o_proj.weight', 'layers.10.self_attn.q_proj.weight', 'layers.10.self_attn.v_proj.weight', 'layers.11.input_layernorm.weight', 'layers.11.mlp.down_proj.weight', 'layers.11.mlp.gate_proj.weight', 'layers.11.mlp.up_proj.weight', 'layers.11.post_attention_layernorm.weight', 'layers.11.self_attn.k_proj.weight', 'layers.11.self_attn.o_proj.weight', 'layers.11.self_attn.q_proj.weight', 'layers.11.self_attn.v_proj.weight', 'layers.12.input_layernorm.weight', 'layers.12.mlp.down_proj.weight', 'layers.12.mlp.gate_proj.weight', 'layers.12.mlp.up_proj.weight', 'layers.12.post_attention_layernorm.weight', 'layers.12.self_attn.k_proj.weight', 'layers.12.self_attn.o_proj.weight', 'layers.12.self_attn.q_proj.weight', 'layers.12.self_attn.v_proj.weight', 'layers.13.input_layernorm.weight', 'layers.13.mlp.down_proj.weight', 'layers.13.mlp.gate_proj.weight', 'layers.13.mlp.up_proj.weight', 'layers.13.post_attention_layernorm.weight', 'layers.13.self_attn.k_proj.weight', 'layers.13.self_attn.o_proj.weight', 'layers.13.self_attn.q_proj.weight', 'layers.13.self_attn.v_proj.weight', 'layers.14.input_layernorm.weight', 'layers.14.mlp.down_proj.weight', 'layers.14.mlp.gate_proj.weight', 'layers.14.mlp.up_proj.weight', 'layers.14.post_attention_layernorm.weight', 'layers.14.self_attn.k_proj.weight', 'layers.14.self_attn.o_proj.weight', 'layers.14.self_attn.q_proj.weight', 'layers.14.self_attn.v_proj.weight', 'layers.15.input_layernorm.weight', 'layers.15.mlp.down_proj.weight', 'layers.15.mlp.gate_proj.weight', 'layers.15.mlp.up_proj.weight', 'layers.15.post_attention_layernorm.weight', 'layers.15.self_attn.k_proj.weight', 'layers.15.self_attn.o_proj.weight', 'layers.15.self_attn.q_proj.weight', 'layers.15.self_attn.v_proj.weight', 'layers.2.input_layernorm.weight', 'layers.2.mlp.down_proj.weight', 'layers.2.mlp.gate_proj.weight', 'layers.2.mlp.up_proj.weight', 'layers.2.post_attention_layernorm.weight', 'layers.2.self_attn.k_proj.weight', 'layers.2.self_attn.o_proj.weight', 'layers.2.self_attn.q_proj.weight', 'layers.2.self_attn.v_proj.weight', 'layers.3.input_layernorm.weight', 'layers.3.mlp.down_proj.weight', 'layers.3.mlp.gate_proj.weight', 'layers.3.mlp.up_proj.weight', 'layers.3.post_attention_layernorm.weight', 'layers.3.self_attn.k_proj.weight', 'layers.3.self_attn.o_proj.weight', 'layers.3.self_attn.q_proj.weight', 'layers.3.self_attn.v_proj.weight', 'layers.4.input_layernorm.weight', 'layers.4.mlp.down_proj.weight', 'layers.4.mlp.gate_proj.weight', 'layers.4.mlp.up_proj.weight', 'layers.4.post_attention_layernorm.weight', 'layers.4.self_attn.k_proj.weight', 'layers.4.self_attn.o_proj.weight', 'layers.4.self_attn.q_proj.weight', 'layers.4.self_attn.v_proj.weight', 'layers.5.input_layernorm.weight', 'layers.5.mlp.down_proj.weight', 'layers.5.mlp.gate_proj.weight', 'layers.5.mlp.up_proj.weight', 'layers.5.post_attention_layernorm.weight', 'layers.5.self_attn.k_proj.weight', 'layers.5.self_attn.o_proj.weight', 'layers.5.self_attn.q_proj.weight', 'layers.5.self_attn.v_proj.weight', 'layers.6.input_layernorm.weight', 'layers.6.mlp.down_proj.weight', 'layers.6.mlp.gate_proj.weight', 'layers.6.mlp.up_proj.weight', 'layers.6.post_attention_layernorm.weight', 'layers.6.self_attn.k_proj.weight', 'layers.6.self_attn.o_proj.weight', 'layers.6.self_attn.q_proj.weight', 'layers.6.self_attn.v_proj.weight', 'layers.7.input_layernorm.weight', 'layers.7.mlp.down_proj.weight', 'layers.7.mlp.gate_proj.weight', 'layers.7.mlp.up_proj.weight', 'layers.7.post_attention_layernorm.weight', 'layers.7.self_attn.k_proj.weight', 'layers.7.self_attn.o_proj.weight', 'layers.7.self_attn.q_proj.weight', 'layers.7.self_attn.v_proj.weight', 'layers.8.input_layernorm.weight', 'layers.8.mlp.down_proj.weight', 'layers.8.mlp.gate_proj.weight', 'layers.8.mlp.up_proj.weight', 'layers.8.post_attention_layernorm.weight', 'layers.8.self_attn.k_proj.weight', 'layers.8.self_attn.o_proj.weight', 'layers.8.self_attn.q_proj.weight', 'layers.8.self_attn.v_proj.weight', 'layers.9.input_layernorm.weight', 'layers.9.mlp.down_proj.weight', 'layers.9.mlp.gate_proj.weight', 'layers.9.mlp.up_proj.weight', 'layers.9.post_attention_layernorm.weight', 'layers.9.self_attn.k_proj.weight', 'layers.9.self_attn.o_proj.weight', 'layers.9.self_attn.q_proj.weight', 'layers.9.self_attn.v_proj.weight', 'norm.weight', 'qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model with question answering layer loaded\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# setting the padding token manually:\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"tokenizer loaded\")\n",
    "\n",
    "# model without question-answering layer:\n",
    "base_model = AutoModel.from_pretrained(model_name)\n",
    "print(\"base model loaded\")\n",
    "\n",
    "# model with question-answering layer:\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "print(\"model with question answering layer loaded\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting the model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:53:03.031802Z",
     "iopub.status.busy": "2024-11-22T21:53:03.030934Z",
     "iopub.status.idle": "2024-11-22T21:53:03.039011Z",
     "shell.execute_reply": "2024-11-22T21:53:03.037780Z",
     "shell.execute_reply.started": "2024-11-22T21:53:03.031751Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# function to count model parameters:\n",
    "def print_model_parameters_tabular(model):\n",
    "    parameters = []\n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "\n",
    "    # Collect model parameter details\n",
    "    for name, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        total_params += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "        parameters.append([name, list(param.shape), param.requires_grad, f\"{num_params:,}\"])\n",
    "\n",
    "    # Print parameter details in a more compact, line-by-line format\n",
    "    print(f\"Model Parameters for {type(model).__name__}:\\n\")\n",
    "    for param in parameters:\n",
    "        name, shape, requires_grad, num_elements = param\n",
    "        print(f\"Parameter Name: {name}\")\n",
    "        print(f\"  Shape: {shape}\")\n",
    "        print(f\"  Requires Grad: {requires_grad}\")\n",
    "        print(f\"  Total Elements: {num_elements}\")\n",
    "        print(\"-\" * 50)  # Separator line for clarity\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nTotal Parameters: {total_params:,}\")\n",
    "    print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "    print(f\"Non-Trainable Parameters: {total_params - trainable_params:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without question-answering layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:53:07.652458Z",
     "iopub.status.busy": "2024-11-22T21:53:07.651818Z",
     "iopub.status.idle": "2024-11-22T21:53:07.662438Z",
     "shell.execute_reply": "2024-11-22T21:53:07.661510Z",
     "shell.execute_reply.started": "2024-11-22T21:53:07.652421Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters for LlamaModel:\n",
      "\n",
      "Parameter Name: embed_tokens.weight\n",
      "  Shape: [128256, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 262,668,288\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.0.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.0.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.0.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.0.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.0.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.0.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.0.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.0.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.0.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.1.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.1.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.1.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.1.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.1.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.1.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.1.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.1.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.1.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.2.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.2.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.2.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.2.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.2.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.2.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.2.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.2.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.2.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.3.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.3.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.3.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.3.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.3.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.3.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.3.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.3.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.3.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.4.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.4.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.4.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.4.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.4.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.4.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.4.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.4.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.4.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.5.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.5.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.5.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.5.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.5.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.5.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.5.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.5.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.5.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.6.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.6.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.6.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.6.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.6.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.6.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.6.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.6.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.6.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.7.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.7.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.7.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.7.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.7.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.7.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.7.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.7.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.7.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.8.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.8.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.8.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.8.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.8.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.8.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.8.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.8.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.8.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.9.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.9.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.9.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.9.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.9.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.9.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.9.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.9.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.9.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.10.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.10.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.10.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.10.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.10.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.10.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.10.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.10.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.10.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.11.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.11.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.11.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.11.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.11.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.11.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.11.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.11.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.11.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.12.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.12.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.12.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.12.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.12.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.12.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.12.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.12.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.12.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.13.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.13.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.13.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.13.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.13.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.13.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.13.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.13.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.13.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.14.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.14.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.14.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.14.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.14.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.14.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.14.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.14.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.14.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.15.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.15.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.15.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.15.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.15.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.15.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.15.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.15.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: layers.15.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: norm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "\n",
      "Total Parameters: 1,235,814,400\n",
      "Trainable Parameters: 1,235,814,400\n",
      "Non-Trainable Parameters: 0\n"
     ]
    }
   ],
   "source": [
    "# printing parameters of base model:\n",
    "print_model_parameters_tabular(base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With question-answering layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:53:18.106628Z",
     "iopub.status.busy": "2024-11-22T21:53:18.105761Z",
     "iopub.status.idle": "2024-11-22T21:53:18.117229Z",
     "shell.execute_reply": "2024-11-22T21:53:18.116188Z",
     "shell.execute_reply.started": "2024-11-22T21:53:18.106585Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters for LlamaForQuestionAnswering:\n",
      "\n",
      "Parameter Name: transformer.embed_tokens.weight\n",
      "  Shape: [128256, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 262,668,288\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.0.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.0.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.0.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.0.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.0.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.0.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.0.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.0.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.0.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.1.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.1.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.1.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.1.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.1.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.1.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.1.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.1.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.1.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.2.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.2.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.2.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.2.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.2.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.2.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.2.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.2.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.2.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.3.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.3.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.3.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.3.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.3.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.3.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.3.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.3.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.3.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.4.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.4.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.4.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.4.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.4.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.4.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.4.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.4.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.4.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.5.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.5.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.5.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.5.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.5.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.5.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.5.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.5.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.5.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.6.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.6.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.6.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.6.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.6.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.6.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.6.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.6.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.6.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.7.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.7.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.7.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.7.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.7.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.7.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.7.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.7.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.7.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.8.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.8.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.8.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.8.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.8.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.8.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.8.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.8.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.8.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.9.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.9.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.9.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.9.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.9.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.9.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.9.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.9.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.9.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.10.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.10.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.10.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.10.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.10.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.10.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.10.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.10.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.10.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.11.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.11.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.11.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.11.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.11.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.11.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.11.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.11.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.11.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.12.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.12.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.12.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.12.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.12.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.12.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.12.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.12.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.12.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.13.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.13.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.13.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.13.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.13.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.13.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.13.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.13.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.13.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.14.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.14.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.14.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.14.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.14.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.14.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.14.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.14.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.14.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.15.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.15.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.15.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.15.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.15.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.15.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.15.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.15.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.layers.15.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: transformer.norm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: qa_outputs.weight\n",
      "  Shape: [2, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,096\n",
      "--------------------------------------------------\n",
      "Parameter Name: qa_outputs.bias\n",
      "  Shape: [2]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2\n",
      "--------------------------------------------------\n",
      "\n",
      "Total Parameters: 1,235,818,498\n",
      "Trainable Parameters: 1,235,818,498\n",
      "Non-Trainable Parameters: 0\n"
     ]
    }
   ],
   "source": [
    "print_model_parameters_tabular(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As can be seen, the addition of a question-answering head to make the model suitable for a question-answering task adds 4098 extra parameters to the base model. During fine-tuning, we will only be training these parameters and not change the parameters of the base model (due to memory and computation constraints - and also, the relevant parameters for question-answering are only those added in the final layer).\n",
    "\n",
    "### We will also check the number of parameters after fine-tuning, to show that fine-tuning by itself doesn't alter the number of parameters, but adding an extra layer does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:53:36.074575Z",
     "iopub.status.busy": "2024-11-22T21:53:36.073902Z",
     "iopub.status.idle": "2024-11-22T21:53:37.727325Z",
     "shell.execute_reply": "2024-11-22T21:53:37.726437Z",
     "shell.execute_reply.started": "2024-11-22T21:53:36.074537Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "    num_rows: 87599\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 70079\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 17520\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"rajpurkar/squad_v2\", split=\"train\")\n",
    "print(dataset)\n",
    "\n",
    "# splitting on the default 'train' split as asked, with a train-test ratio of 80:20 and random_state = 1 \n",
    "split_dataset = dataset.train_test_split(test_size=0.2, seed=1)\n",
    "print(split_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:53:41.900396Z",
     "iopub.status.busy": "2024-11-22T21:53:41.899623Z",
     "iopub.status.idle": "2024-11-22T21:53:41.905477Z",
     "shell.execute_reply": "2024-11-22T21:53:41.904450Z",
     "shell.execute_reply.started": "2024-11-22T21:53:41.900359Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '573056f1069b531400832087', 'title': 'Windows_8', 'context': 'In June 2014, state broadcaster China Central Television (CCTV) broadcast a news story further characterizing Windows 8 as a threat to national security. The story featured an interview with Ni Guangnan, who stated that operating systems could aggregate \"sensitive user information\" that could be used to \"understand the conditions and activities of our national economy and society\", and alleged that per documents leaked by Edward Snowden, the U.S. government had worked with Microsoft to retrieve encrypted information. Yang Min, a computer scientist at Fudan University, also stated that \"the security features of Windows 8 are basically to the benefit of Microsoft, allowing them control of the users\\' data, and that poses a big challenge to the national strategy for information security.\" Microsoft denied the claims in a number of posts on the Chinese social network Sina Weibo, which stated that the company had never \"assisted any government in an attack of another government or clients\" or provided client data to the U.S. government, never \"provided any government the authority to directly visit\" or placed any backdoors in its products and services, and that it had never concealed government requests for client data.', 'question': 'Who claimed that the Window 8 OS could gather sensitive user information?', 'answers': {'text': ['Ni Guangnan'], 'answer_start': [191]}}\n"
     ]
    }
   ],
   "source": [
    "# training/testing sample:\n",
    "print(split_dataset['train'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:53:57.265959Z",
     "iopub.status.busy": "2024-11-22T21:53:57.265591Z",
     "iopub.status.idle": "2024-11-22T21:53:57.275709Z",
     "shell.execute_reply": "2024-11-22T21:53:57.274670Z",
     "shell.execute_reply.started": "2024-11-22T21:53:57.265926Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# function to tokenize the data (takes the sample printed above as input)\n",
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=128,\n",
    "        truncation=\"only_second\",\n",
    "        stride=64,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    inputs[\"example_id\"] = [examples[\"id\"][i] for i in sample_map]\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while idx < len(sequence_ids) and sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while idx < len(sequence_ids) and sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:54:02.540474Z",
     "iopub.status.busy": "2024-11-22T21:54:02.540114Z",
     "iopub.status.idle": "2024-11-22T21:54:12.997524Z",
     "shell.execute_reply": "2024-11-22T21:54:12.996611Z",
     "shell.execute_reply.started": "2024-11-22T21:54:02.540444Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data tokenized\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98f9ada78c6f4dc18a486b59703c0dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17520 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data tokenized\n"
     ]
    }
   ],
   "source": [
    "# calling the tokenizing function on training + testing samples:\n",
    "tokenized_train_data = split_dataset['train'].map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=split_dataset[\"train\"].column_names, # to make the format suitable for the model\n",
    ")\n",
    "print(\"train data tokenized\")\n",
    "tokenized_test_data = split_dataset['test'].map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=split_dataset[\"test\"].column_names,\n",
    ")\n",
    "print(\"test data tokenized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying tokenized data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:54:22.383992Z",
     "iopub.status.busy": "2024-11-22T21:54:22.383297Z",
     "iopub.status.idle": "2024-11-22T21:54:22.389757Z",
     "shell.execute_reply": "2024-11-22T21:54:22.388615Z",
     "shell.execute_reply.started": "2024-11-22T21:54:22.383956Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [128000, 4599, 574, 23225, 596, 22760, 367, 30, 128000, 1688, 279, 892, 315, 1077, 85045, 11, 279, 3109, 574, 6197, 555, 279, 1254, 343, 10461, 13015, 10425, 27535, 11, 889, 520, 3131, 6244, 264, 8147, 10383, 389, 279, 31205, 79966, 16657, 11, 889, 41013, 389, 1461, 369, 9650, 13, 15274, 480, 7430, 4618, 10171, 430, 279, 9923, 13111, 323, 1716, 1752, 27535, 574, 330, 6519, 290, 2718, 21901, 315, 1077, 439, 568, 2643, 387, 315, 813, 10003, 422, 568, 1047, 832, 498, 323, 23225, 4762, 5602, 1461, 439, 264, 7126, 7216, 13, 6385, 22760, 367, 3952, 2035, 389, 220, 1591, 5651, 220, 10750, 23, 520, 48043, 65555, 13, 6193, 220, 3443, 11, 931, 15613, 3782, 311, 7295, 369, 279, 47674, 13, 3005, 6244, 279, 1176, 46384, 311, 1935], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'example_id': '5726434238643c19005ad3cf', 'start_positions': 99, 'end_positions': 104}\n",
      "{'input_ids': [128000, 3923, 75311, 43398, 2410, 4128, 449, 279, 3552, 3834, 30, 128000, 8538, 2536, 54920, 11602, 7766, 1005, 279, 220, 20, 650, 2410, 8312, 2085, 24435, 304, 264, 6300, 11602, 4009, 11, 902, 11903, 43398, 2410, 4128, 449, 279, 3552, 3834, 13, 4314, 527, 6118, 2663, 11602, 48679, 8032, 84247, 4460, 60, 26379, 2997, 11602, 41503, 13939, 13001, 11, 7359, 11, 52589, 7155, 388, 323, 82467, 11, 11863, 5714, 388, 11, 57863, 29302, 75965, 11, 323, 1524, 57863, 58668, 50552, 13, 763, 1455, 5157, 11, 1521, 3673, 6782, 912, 7528, 16622, 894, 11, 323, 8617, 527, 539, 5410, 49798, 11602, 7766, 13, 1115, 1253, 5353, 5435, 449, 1063, 19002, 11, 1778, 439, 13633, 2288, 1790, 1510, 323, 34446, 16622, 894, 13, 32499, 311, 279, 34712, 90898, 52377, 11], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'example_id': '572801402ca10214002d9b31', 'start_positions': 18, 'end_positions': 24}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_data[0])\n",
    "print(tokenized_test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:54:26.627715Z",
     "iopub.status.busy": "2024-11-22T21:54:26.626864Z",
     "iopub.status.idle": "2024-11-22T21:54:26.631626Z",
     "shell.execute_reply": "2024-11-22T21:54:26.630650Z",
     "shell.execute_reply.started": "2024-11-22T21:54:26.627681Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# data collator instance:\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining training arguments and creating trainer instance (before fine-tuning for pre-fine-tuning evaluation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:55:07.627440Z",
     "iopub.status.busy": "2024-11-22T21:55:07.627014Z",
     "iopub.status.idle": "2024-11-22T21:55:09.300991Z",
     "shell.execute_reply": "2024-11-22T21:55:09.300019Z",
     "shell.execute_reply.started": "2024-11-22T21:55:07.627408Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForQuestionAnswering(\n",
       "  (transformer): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=2048, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:55:18.459638Z",
     "iopub.status.busy": "2024-11-22T21:55:18.459277Z",
     "iopub.status.idle": "2024-11-22T21:55:18.498403Z",
     "shell.execute_reply": "2024-11-22T21:55:18.497711Z",
     "shell.execute_reply.started": "2024-11-22T21:55:18.459608Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# defining training arguments:\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",     \n",
    "    per_device_eval_batch_size=8, # due to hardware constraints\n",
    "    do_eval=False,                  \n",
    "    logging_dir='./logs',         \n",
    "    report_to=\"none\", \n",
    ")\n",
    "\n",
    "# trainer instance:\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,      \n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to compute various evaluation metrics and compare pre- and post-fine-tuning performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:55:28.669342Z",
     "iopub.status.busy": "2024-11-22T21:55:28.668454Z",
     "iopub.status.idle": "2024-11-22T21:55:28.674835Z",
     "shell.execute_reply": "2024-11-22T21:55:28.673814Z",
     "shell.execute_reply.started": "2024-11-22T21:55:28.669297Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compare_predictions(test_data, squad_dataset_test, start_idx, end_idx, tokenizer):\n",
    "\n",
    "    original_answers_map = {\n",
    "        example[\"id\"]: example[\"answers\"][\"text\"][0] for example in squad_dataset_test\n",
    "    }\n",
    "    comparisons = [\n",
    "        (\n",
    "            original_answers_map[example_id],  # Original answer\n",
    "            tokenizer.decode(\n",
    "                test_data[\"input_ids\"][i][start_idx[i]:end_idx[i] + 1]\n",
    "            ).strip() if start_idx[i] != 0 and end_idx[i] != 0 else \"\"\n",
    "        )\n",
    "        for i, example_id in enumerate(test_data[\"example_id\"])\n",
    "    ]\n",
    "\n",
    "    return comparisons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:55:31.656589Z",
     "iopub.status.busy": "2024-11-22T21:55:31.656240Z",
     "iopub.status.idle": "2024-11-22T21:55:31.660802Z",
     "shell.execute_reply": "2024-11-22T21:55:31.659818Z",
     "shell.execute_reply.started": "2024-11-22T21:55:31.656559Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_average_meteor_scores(references, predictions):\n",
    "    total = 0\n",
    "    count = len(predictions)\n",
    "\n",
    "    for ref, pred in zip(references, predictions):\n",
    "        ref = word_tokenize(ref)\n",
    "        pred = word_tokenize(pred)\n",
    "        score_details = meteor_score([ref], pred)\n",
    "        total += score_details\n",
    "\n",
    "    avg = total / count\n",
    "\n",
    "    return {\n",
    "        \"score\": avg,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:58:10.405442Z",
     "iopub.status.busy": "2024-11-22T21:58:10.404772Z",
     "iopub.status.idle": "2024-11-22T21:58:10.414495Z",
     "shell.execute_reply": "2024-11-22T21:58:10.413571Z",
     "shell.execute_reply.started": "2024-11-22T21:58:10.405407Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_function(test_data, tokenizer, squad_dataset_test, trainer):\n",
    "    predictions, _, _ = trainer.predict(test_data)\n",
    "    start_logits, end_logits = predictions\n",
    "    start_idx = np.argmax(start_logits, axis=1)\n",
    "    end_idx = np.argmax(end_logits, axis=1)\n",
    "\n",
    "    comparisons = compare_predictions(test_data, squad_dataset_test, start_idx, end_idx, tokenizer)\n",
    "    original_answers = [original for original, _ in comparisons]\n",
    "    predicted_answers = [predicted for _, predicted in comparisons]\n",
    "    print(\"hey\")\n",
    "    squad_predictions = [\n",
    "        {\n",
    "            \"id\": str(i),\n",
    "            \"prediction_text\": pred,\n",
    "            \"no_answer_probability\": 1.0 if len(pred.strip()) == 0 else 0.0,\n",
    "        }\n",
    "        for i, pred in enumerate(predicted_answers)\n",
    "    ]\n",
    "\n",
    "    squad_references = [\n",
    "        {\"id\": str(i), \"answers\": {\"text\": [orig], \"answer_start\": []}}\n",
    "        for i, orig in enumerate(original_answers)\n",
    "    ]\n",
    "    squad_metric = evaluate.load(\"squad_v2\")\n",
    "    bleu_metric = evaluate.load(\"bleu\")\n",
    "    rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "    squad_results = squad_metric.compute(\n",
    "        predictions=squad_predictions,\n",
    "        references=squad_references\n",
    "    )\n",
    "    \n",
    "    bleu_results = bleu_metric.compute(\n",
    "        predictions=predicted_answers,\n",
    "        references=[[orig] for orig in original_answers]\n",
    "    )\n",
    "    \n",
    "    rouge_results = rouge_metric.compute(\n",
    "        predictions=predicted_answers,\n",
    "        references=original_answers\n",
    "    )\n",
    "    \n",
    "    meteor_results = compute_average_meteor_scores(original_answers, predicted_answers)\n",
    "\n",
    "    # final output - returning all metrics:\n",
    "    results = {\n",
    "        \"squad_v2\": squad_results,\n",
    "        \"bleu\": bleu_results[\"bleu\"],\n",
    "        \"rouge-2\": rouge_results[\"rouge2\"],\n",
    "        \"rouge-L\": rouge_results[\"rougeL\"],\n",
    "        \"rouge-1\": rouge_results[\"rouge1\"],\n",
    "        \"meteor\": meteor_results,\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:58:14.911882Z",
     "iopub.status.busy": "2024-11-22T21:58:14.911449Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "results_before = evaluate_function(tokenized_test_data, tokenizer, split_dataset[\"test\"], trainer)\n",
    "print(results_before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freezing all model parameters except the last question-answering layer added to the base model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for param in model.transformer.parameters():\n",
    "    param.requires_grad = False\n",
    "# verify if the top layer is still trainable\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)# Define the training arguments\n",
    "\n",
    "# training arguments:\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./post_finetuning_results\",            \n",
    "    evaluation_strategy=\"no\",                    \n",
    "    learning_rate=2e-5,                          \n",
    "    per_device_train_batch_size=8,  # set to a smaller number due to memory and compute constraints\n",
    "    per_device_eval_batch_size=8,                \n",
    "    num_train_epochs=3,                          \n",
    "    weight_decay=0.01,                           \n",
    "    logging_dir=\"./logs\",                        \n",
    "    report_to=\"none\",                      \n",
    "    save_strategy=\"epoch\",                      \n",
    "    fp16=True,                                  \n",
    "    logging_steps=10,                           \n",
    "    lr_scheduler_type=\"linear\",                 \n",
    "    load_best_model_at_end=False,                \n",
    ")\n",
    "\n",
    "# instantiating trainer for fine-tuning\n",
    "trainer = Trainer(\n",
    "    model=model,       \n",
    "    args=training_args,               \n",
    "    train_dataset=tokenized_train_data, \n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results_after = evaluate_function(tokenized_test_data, tokenizer, split_dataset[\"test\"], trainer)\n",
    "print(results_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def create_single_table(results_before, results_after):\n",
    "    def format_value(value):\n",
    "        \"\"\"Helper function to format values consistently.\"\"\"\n",
    "        if isinstance(value, float):\n",
    "            return f\"{value:.4f}\"\n",
    "        return value if value is not None else \"N/A\"\n",
    "\n",
    "    # Collect all keys\n",
    "    all_keys = set(results_before.keys()).union(results_after.keys())\n",
    "\n",
    "    # Prepare data for the table\n",
    "    table_data = []\n",
    "    for key in sorted(all_keys):\n",
    "        if isinstance(results_before.get(key), dict) or isinstance(results_after.get(key), dict):\n",
    "            # Handle nested dictionaries\n",
    "            sub_keys = set(results_before.get(key, {}).keys()).union(results_after.get(key, {}).keys())\n",
    "            for sub_key in sorted(sub_keys):\n",
    "                before_value = format_value(results_before.get(key, {}).get(sub_key))\n",
    "                after_value = format_value(results_after.get(key, {}).get(sub_key))\n",
    "                table_data.append([key, sub_key, before_value, after_value])\n",
    "        else:\n",
    "            # Handle scalar values\n",
    "            before_value = format_value(results_before.get(key))\n",
    "            after_value = format_value(results_after.get(key))\n",
    "            table_data.append([key, \"-\", before_value, after_value])\n",
    "\n",
    "    # Create the figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(10, len(table_data) * 0.5))\n",
    "    ax.axis(\"tight\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    # Add a title\n",
    "    title = \"Comparison of Results Before and After Fine-Tuning\"\n",
    "    fig.suptitle(title, fontsize=14, fontweight=\"bold\", y=0.98)\n",
    "\n",
    "    # Create the table\n",
    "    column_labels = [\"Metric\", \"Sub-Metric\", \"Before Fine-Tuning\", \"After Fine-Tuning\"]\n",
    "    table = ax.table(\n",
    "        cellText=table_data,\n",
    "        colLabels=column_labels,\n",
    "        cellLoc=\"center\",\n",
    "        loc=\"center\",\n",
    "    )\n",
    "\n",
    "    # Style the table\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.auto_set_column_width(col=list(range(len(column_labels))))\n",
    "\n",
    "    # Display the table\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "repo_name = \"jiya14desai/Llama-3.2-1B_fine-tuned_on_questionanswering\"\n",
    "model.push_to_hub(repo_name)\n",
    "print(f\"Model uploaded to Hugging Face Hub under repository: {repo_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "\n",
    "pretrained_params = count_parameters(model)\n",
    "finetuned_params = count_parameters(trainer.model)\n",
    "\n",
    "print(f\"Number of parameters in the pre-trained model: {pretrained_params}\")\n",
    "print(f\"Number of parameters in the fine-tuned model: {finetuned_params}\")\n",
    "if pretrained_params == finetuned_params:\n",
    "    print(\"The number of parameters remains the same after fine-tuning.\")\n",
    "else:\n",
    "    print(\"The number of parameters differs after fine-tuning.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
