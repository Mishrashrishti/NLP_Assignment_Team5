{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging into huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-21T21:42:46.190804Z",
     "iopub.status.busy": "2024-11-21T21:42:46.189964Z",
     "iopub.status.idle": "2024-11-21T21:42:46.670383Z",
     "shell.execute_reply": "2024-11-21T21:42:46.669519Z",
     "shell.execute_reply.started": "2024-11-21T21:42:46.190767Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# login to huggingface snippet\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enabling GPU usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:42:49.594243Z",
     "iopub.status.busy": "2024-11-21T21:42:49.593915Z",
     "iopub.status.idle": "2024-11-21T21:42:52.558382Z",
     "shell.execute_reply": "2024-11-21T21:42:52.557498Z",
     "shell.execute_reply.started": "2024-11-21T21:42:49.594215Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if a GPU is available\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:42:55.518702Z",
     "iopub.status.busy": "2024-11-21T21:42:55.517980Z",
     "iopub.status.idle": "2024-11-21T21:43:09.852038Z",
     "shell.execute_reply": "2024-11-21T21:43:09.851345Z",
     "shell.execute_reply.started": "2024-11-21T21:42:55.518666Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "\n",
    "    AutoTokenizer,\n",
    "\n",
    "    AutoModelForSequenceClassification,\n",
    "\n",
    "    Trainer,\n",
    "\n",
    "    TrainingArguments,\n",
    "\n",
    "    DataCollatorWithPadding,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Model and the Model's Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:45:42.634358Z",
     "iopub.status.busy": "2024-11-21T21:45:42.633640Z",
     "iopub.status.idle": "2024-11-21T21:45:48.704655Z",
     "shell.execute_reply": "2024-11-21T21:45:48.703654Z",
     "shell.execute_reply.started": "2024-11-21T21:45:42.634325Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model (after adding classification head) and tokenizer loaded.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "\n",
    "# loading model - the Llama-3.2-1B model is not meant for classification, so the AutoModelForSequenceClassification adds a classification head to the loaded model\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Model (after adding classification head) and tokenizer loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the tokenizer padding manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:45:58.168195Z",
     "iopub.status.busy": "2024-11-21T21:45:58.167361Z",
     "iopub.status.idle": "2024-11-21T21:45:58.194853Z",
     "shell.execute_reply": "2024-11-21T21:45:58.193946Z",
     "shell.execute_reply.started": "2024-11-21T21:45:58.168159Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer padding set.\n"
     ]
    }
   ],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Use the eos_token as the pad_token\n",
    "\n",
    "    model.resize_token_embeddings(len(tokenizer))  # Resize embeddings to accommodate new tokens\n",
    "\n",
    "\n",
    "\n",
    "print(\"Tokenizer padding set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Total Model Parameters Before Adding the Classication Layer and Fine-tuning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without the classification head: The LLaMA model, being a language model, is designed to predict the next token in a sequence, not to directly output class labels for tasks like sentiment analysis. To fine-tune LLaMA for classification tasks, such as SST-2, a classification head (a linear layer) must be added to the model. This head maps the contextual embeddings generated by the LLaMA model (specifically from the [CLS] token or its equivalent) into logits corresponding to the target classes. Without this classification layer, the model outputs sequence-level embeddings that are not suitable for computing classification loss or making predictions. Fine-tuning the model with this added layer allows it to adapt its parameters to the classification task, making it capable of generating the necessary class logits for tasks like sentiment analysis.\n",
    "\n",
    "### Without adding this layer, we cannot use the model for a classification task. Hence, for this assignment, when we are comparing the number of parameters before and after fine-tuning, we are considering the model WITHOUT adding the classification layer, and with the classification layer. Fine-tuning by itself does not change the model's parameters. But adding the classification layer adds to the number of parameters, which is needed for fine-tuning the model on classification tasks. Hence, we compared the total model parameters of the base model (calculated below) and the parameters of the fine-tuned model (calculated after fine-tuning). But if we calculate the parameters after adding the layer before fine-tuning and after fine-tuning, the number of parameters will be the same.\n",
    "\n",
    "\n",
    "\n",
    "## In short, fine-tuning by itself does not change the model's parameters, but adding the classification layer (which is needed for fine-tuning for a classification task) does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:46:51.632577Z",
     "iopub.status.busy": "2024-11-21T21:46:51.632217Z",
     "iopub.status.idle": "2024-11-21T21:46:54.055247Z",
     "shell.execute_reply": "2024-11-21T21:46:54.054262Z",
     "shell.execute_reply.started": "2024-11-21T21:46:51.632547Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "535f51e426a647ceb7a250207b1fccbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters for LlamaForCausalLM:\n",
      "\n",
      "Parameter Name: model.embed_tokens.weight\n",
      "  Shape: [128256, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 262,668,288\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.0.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.0.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.0.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.0.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.0.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.0.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.0.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.0.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.0.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.1.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.1.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.1.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.1.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.1.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.1.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.1.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.1.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.1.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.2.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.2.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.2.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.2.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.2.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.2.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.2.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.2.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.2.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.3.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.3.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.3.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.3.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.3.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.3.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.3.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.3.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.3.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.4.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.4.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.4.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.4.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.4.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.4.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.4.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.4.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.4.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.5.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.5.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.5.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.5.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.5.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.5.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.5.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.5.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.5.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.6.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.6.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.6.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.6.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.6.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.6.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.6.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.6.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.6.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.7.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.7.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.7.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.7.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.7.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.7.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.7.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.7.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.7.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.8.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.8.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.8.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.8.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.8.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.8.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.8.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.8.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.8.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.9.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.9.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.9.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.9.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.9.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.9.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.9.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.9.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.9.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.10.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.10.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.10.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.10.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.10.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.10.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.10.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.10.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.10.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.11.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.11.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.11.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.11.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.11.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.11.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.11.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.11.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.11.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.12.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.12.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.12.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.12.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.12.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.12.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.12.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.12.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.12.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.13.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.13.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.13.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.13.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.13.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.13.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.13.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.13.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.13.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.14.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.14.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.14.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.14.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.14.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.14.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.14.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.14.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.14.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.15.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.15.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.15.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.15.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.15.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.15.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.15.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.15.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.15.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.norm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "\n",
      "Total Parameters: 1,235,814,400\n",
      "Trainable Parameters: 1,235,814,400\n",
      "Non-Trainable Parameters: 0\n",
      "Model parameters before fine-tuning (before adding classification head) printed.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "\n",
    "# this is the original model - before we added the classification layer:\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name) \n",
    "\n",
    "\n",
    "\n",
    "def print_model_parameters_tabular(model):\n",
    "\n",
    "    parameters = []\n",
    "\n",
    "    total_params = 0\n",
    "\n",
    "    trainable_params = 0\n",
    "\n",
    "\n",
    "\n",
    "    # Collect model parameter details\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "\n",
    "        num_params = param.numel()\n",
    "\n",
    "        total_params += num_params\n",
    "\n",
    "        if param.requires_grad:\n",
    "\n",
    "            trainable_params += num_params\n",
    "\n",
    "        parameters.append([name, list(param.shape), param.requires_grad, f\"{num_params:,}\"])\n",
    "\n",
    "\n",
    "\n",
    "    # Print parameter details in a more compact, line-by-line format\n",
    "\n",
    "    print(f\"Model Parameters for {type(model).__name__}:\\n\")\n",
    "\n",
    "    for param in parameters:\n",
    "\n",
    "        name, shape, requires_grad, num_elements = param\n",
    "\n",
    "        print(f\"Parameter Name: {name}\")\n",
    "\n",
    "        print(f\"  Shape: {shape}\")\n",
    "\n",
    "        print(f\"  Requires Grad: {requires_grad}\")\n",
    "\n",
    "        print(f\"  Total Elements: {num_elements}\")\n",
    "\n",
    "        print(\"-\" * 50)  # Separator line for clarity\n",
    "\n",
    "    \n",
    "\n",
    "    # Print summary\n",
    "\n",
    "    print(f\"\\nTotal Parameters: {total_params:,}\")\n",
    "\n",
    "    print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "\n",
    "    print(f\"Non-Trainable Parameters: {total_params - trainable_params:,}\")\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "print_model_parameters_tabular(base_model)\n",
    "\n",
    "print(\"Model parameters before fine-tuning (before adding classification head) printed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Total Model Parameters Without the Classification Layer (i.e., before fine-tuning the model for classification): 1235814400.\n",
    "\n",
    "Yes, this number matches with the parameters reported in the official documentation of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the SST-2 Dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Creating test and train splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:48:00.428148Z",
     "iopub.status.busy": "2024-11-21T21:48:00.427325Z",
     "iopub.status.idle": "2024-11-21T21:48:00.461662Z",
     "shell.execute_reply": "2024-11-21T21:48:00.460759Z",
     "shell.execute_reply.started": "2024-11-21T21:48:00.428114Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:  Dataset({\n",
      "    features: ['sentence', 'label', 'idx'],\n",
      "    num_rows: 53879\n",
      "})\n",
      "Testing data:  Dataset({\n",
      "    features: ['sentence', 'label', 'idx'],\n",
      "    num_rows: 13470\n",
      "})\n",
      "Dataset divided into training and testing portions.\n"
     ]
    }
   ],
   "source": [
    "# Perform an 80:20 train-test split\n",
    "\n",
    "split_dataset = dataset[\"train\"].train_test_split(test_size=0.2, seed=1)\n",
    "\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "\n",
    "test_dataset = split_dataset[\"test\"]\n",
    "\n",
    "eval_dataset = dataset[\"validation\"]\n",
    "\n",
    "print(\"Training data: \", train_dataset)\n",
    "\n",
    "print(\"Testing data: \", test_dataset)\n",
    "\n",
    "print(\"Dataset divided into training and testing portions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Tokenizing the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:48:24.955301Z",
     "iopub.status.busy": "2024-11-21T21:48:24.954536Z",
     "iopub.status.idle": "2024-11-21T21:48:29.691036Z",
     "shell.execute_reply": "2024-11-21T21:48:29.690233Z",
     "shell.execute_reply.started": "2024-11-21T21:48:24.955267Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2105e7dc1fcb463bb43b5a165d329bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/53879 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86cbed95497043ce9b894bba8135ca93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13470 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b0d61c44a146b887306a3861a02f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized training data:  Dataset({\n",
      "    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 53879\n",
      "})\n",
      "Tokenized testing data:  Dataset({\n",
      "    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 13470\n",
      "})\n",
      "Data tokenized.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "\n",
    "def preprocess_function(examples):\n",
    "\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True, max_length=128)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "eval_dataset = eval_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Tokenized training data: \", train_dataset)\n",
    "\n",
    "print(\"Tokenized testing data: \", test_dataset)\n",
    "\n",
    "print(\"Data tokenized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:48:51.372060Z",
     "iopub.status.busy": "2024-11-21T21:48:51.371715Z",
     "iopub.status.idle": "2024-11-21T21:48:51.377186Z",
     "shell.execute_reply": "2024-11-21T21:48:51.376179Z",
     "shell.execute_reply.started": "2024-11-21T21:48:51.372032Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiated data collator.\n"
     ]
    }
   ],
   "source": [
    "# Data collator for padding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "print(\"Instantiated data collator.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Function to Compute Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:49:21.168308Z",
     "iopub.status.busy": "2024-11-21T21:49:21.167670Z",
     "iopub.status.idle": "2024-11-21T21:49:21.174019Z",
     "shell.execute_reply": "2024-11-21T21:49:21.173082Z",
     "shell.execute_reply.started": "2024-11-21T21:49:21.168272Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function to compute metrics written.\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"binary\")\n",
    "\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "\n",
    "\n",
    "print(\"Function to compute metrics written.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shifting to GPU Before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:49:33.969420Z",
     "iopub.status.busy": "2024-11-21T21:49:33.968700Z",
     "iopub.status.idle": "2024-11-21T21:49:35.582722Z",
     "shell.execute_reply": "2024-11-21T21:49:35.581869Z",
     "shell.execute_reply.started": "2024-11-21T21:49:33.969367Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU in use for training.\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "print(\"GPU in use for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model's performance before fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:49:40.416125Z",
     "iopub.status.busy": "2024-11-21T21:49:40.415743Z",
     "iopub.status.idle": "2024-11-21T21:49:50.062374Z",
     "shell.execute_reply": "2024-11-21T21:49:50.061225Z",
     "shell.execute_reply.started": "2024-11-21T21:49:40.416095Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.25.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:50:11.016365Z",
     "iopub.status.busy": "2024-11-21T21:50:11.015652Z",
     "iopub.status.idle": "2024-11-21T21:50:11.020677Z",
     "shell.execute_reply": "2024-11-21T21:50:11.019707Z",
     "shell.execute_reply.started": "2024-11-21T21:50:11.016332Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported evaluate.\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "print(\"Imported evaluate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:51:03.655073Z",
     "iopub.status.busy": "2024-11-21T21:51:03.654698Z",
     "iopub.status.idle": "2024-11-21T21:55:25.996509Z",
     "shell.execute_reply": "2024-11-21T21:55:25.995607Z",
     "shell.execute_reply.started": "2024-11-21T21:51:03.655040Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13470' max='13470' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13470/13470 04:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7600301504135132, 'eval_model_preparation_time': 0.003, 'eval_accuracy': 0.4414996288047513, 'eval_precision': 0.5531914893617021, 'eval_recall': 0.006902960307978229, 'eval_f1': 0.01363576766749705, 'eval_runtime': 262.2883, 'eval_samples_per_second': 51.356, 'eval_steps_per_second': 51.356}\n",
      "Model evaluated without fine-tuning on test data and results stored for comparison with performance of fine-tuned model.\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "\n",
    "    output_dir=\"./results\",          # Directory for model checkpoints\n",
    "\n",
    "    per_device_eval_batch_size=1,  # Evaluation batch size\n",
    "\n",
    "    do_eval=True,                   # Perform evaluation\n",
    "\n",
    "    logging_dir='./logs',           # Logging directory\n",
    "\n",
    "    report_to=\"none\",               # Disable reporting to other systems (e.g., WandB)\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Create the Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "\n",
    "    model=model,      # Your model with the classification head\n",
    "\n",
    "    args=training_args,              # Training arguments\n",
    "\n",
    "    eval_dataset=test_dataset,          # Evaluation dataset\n",
    "\n",
    "    tokenizer=tokenizer,\n",
    "\n",
    "    data_collator=data_collator,\n",
    "\n",
    "    compute_metrics=compute_metrics, # Metrics function\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "\n",
    "results = trainer.evaluate()\n",
    "\n",
    "\n",
    "\n",
    "# Print the metrics\n",
    "\n",
    "print(results)\n",
    "\n",
    "# these results will be used for comparison with the model performance after fine-tuning\n",
    "\n",
    "\n",
    "\n",
    "print(\"Model evaluated without fine-tuning on test data and results stored for comparison with performance of fine-tuned model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freezing all Model Layers Except the Classification Layer Before Fine-tuning\n",
    "\n",
    "(not changing the parameters of the pretrained model due to computation and memory constraints - so we will only be changing the 4096 parameters of the classification layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:56:07.081932Z",
     "iopub.status.busy": "2024-11-21T21:56:07.081556Z",
     "iopub.status.idle": "2024-11-21T21:56:07.088074Z",
     "shell.execute_reply": "2024-11-21T21:56:07.087179Z",
     "shell.execute_reply.started": "2024-11-21T21:56:07.081898Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All parameters except classification layer parameters frozen.\n"
     ]
    }
   ],
   "source": [
    "# Freeze all layers of the base model\n",
    "\n",
    "for param in model.model.parameters():\n",
    "\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "# Keep the classification head (score layer) trainable\n",
    "\n",
    "for param in model.score.parameters():\n",
    "\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "\n",
    "print(\"All parameters except classification layer parameters frozen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:56:21.806576Z",
     "iopub.status.busy": "2024-11-21T21:56:21.805974Z",
     "iopub.status.idle": "2024-11-21T21:56:22.548747Z",
     "shell.execute_reply": "2024-11-21T21:56:22.547911Z",
     "shell.execute_reply.started": "2024-11-21T21:56:21.806543Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialised.\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "\n",
    "    output_dir=\"./results\",\n",
    "\n",
    "    evaluation_strategy=\"epoch\",\n",
    "\n",
    "    save_strategy=\"epoch\",\n",
    "\n",
    "    learning_rate=2e-5,\n",
    "\n",
    "    per_device_train_batch_size=1, # batch size is kept 1 due to computation and memory constraints\n",
    "\n",
    "    per_device_eval_batch_size=1,\n",
    "\n",
    "    num_train_epochs=3,\n",
    "\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    logging_dir=\"./logs\",\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Initialize Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "\n",
    "    model=model,\n",
    "\n",
    "    args=training_args,\n",
    "\n",
    "    train_dataset=train_dataset,\n",
    "\n",
    "    eval_dataset=eval_dataset,\n",
    "\n",
    "    tokenizer=tokenizer,\n",
    "\n",
    "    data_collator=data_collator,\n",
    "\n",
    "    compute_metrics=compute_metrics,\n",
    "\n",
    ")\n",
    "\n",
    "print(\"Trainer initialised.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:58:15.340426Z",
     "iopub.status.busy": "2024-11-21T21:58:15.340016Z",
     "iopub.status.idle": "2024-11-21T23:02:14.448732Z",
     "shell.execute_reply": "2024-11-21T23:02:14.447808Z",
     "shell.execute_reply.started": "2024-11-21T21:58:15.340393Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f89f64a6144476c8edc29c9d69f435f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011115281066668103, max=1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20241121_215844-44q71s57</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jiya-desai-indian-institute-of-technology-gandhinagar/huggingface/runs/44q71s57' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/jiya-desai-indian-institute-of-technology-gandhinagar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jiya-desai-indian-institute-of-technology-gandhinagar/huggingface' target=\"_blank\">https://wandb.ai/jiya-desai-indian-institute-of-technology-gandhinagar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jiya-desai-indian-institute-of-technology-gandhinagar/huggingface/runs/44q71s57' target=\"_blank\">https://wandb.ai/jiya-desai-indian-institute-of-technology-gandhinagar/huggingface/runs/44q71s57</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='161637' max='161637' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [161637/161637 1:03:27, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.555700</td>\n",
       "      <td>0.604033</td>\n",
       "      <td>0.819954</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>0.835586</td>\n",
       "      <td>0.825362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.606700</td>\n",
       "      <td>0.543783</td>\n",
       "      <td>0.848624</td>\n",
       "      <td>0.873206</td>\n",
       "      <td>0.822072</td>\n",
       "      <td>0.846868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.505800</td>\n",
       "      <td>0.527815</td>\n",
       "      <td>0.857798</td>\n",
       "      <td>0.873832</td>\n",
       "      <td>0.842342</td>\n",
       "      <td>0.857798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fine-tuned.\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"Model fine-tuned.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing model performance before and after fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T23:07:37.447306Z",
     "iopub.status.busy": "2024-11-21T23:07:37.446894Z",
     "iopub.status.idle": "2024-11-21T23:07:50.947158Z",
     "shell.execute_reply": "2024-11-21T23:07:50.945927Z",
     "shell.execute_reply.started": "2024-11-21T23:07:37.447271Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved in working directory.\n",
      "\n",
      "Comparison Results:\n",
      "| Metric         |   Before Fine-Tuning |   After Fine-Tuning |\n",
      "|:---------------|---------------------:|--------------------:|\n",
      "| eval_accuracy  |           0.4415     |            0.857798 |\n",
      "| eval_precision |           0.553191   |            0.873832 |\n",
      "| eval_recall    |           0.00690296 |            0.842342 |\n",
      "| eval_f1        |           0.0136358  |            0.857798 |\n",
      "Comparison before and after fine-tuning done.\n"
     ]
    }
   ],
   "source": [
    "#saving the model first:\n",
    "\n",
    "trainer.save_model('./finetuned_model_sst2')\n",
    "\n",
    "print(\"model saved in working directory.\")\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "\n",
    "# Your evaluation results dictionaries\n",
    "\n",
    "evaluation_results_before = {\n",
    "\n",
    "    'eval_accuracy': 0.4414996288047513, \n",
    "\n",
    "    'eval_precision': 0.5531914893617021, \n",
    "\n",
    "    'eval_recall': 0.006902960307978229, \n",
    "\n",
    "    'eval_f1': 0.01363576766749705\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "evaluation_results_after_fine_tuning = {\n",
    "\n",
    "    'fine_tune_accuracy': 0.857798, \n",
    "\n",
    "    'fine_tune_precision': 0.873832, \n",
    "\n",
    "    'fine_tune_recall': 0.842342, \n",
    "\n",
    "    'fine_tune_f1': 0.857798\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Convert dictionaries to DataFrames\n",
    "\n",
    "df_before = pd.DataFrame(list(evaluation_results_before.items()), columns=[\"Metric\", \"Before Fine-Tuning\"])\n",
    "\n",
    "df_after = pd.DataFrame(list(evaluation_results_after_fine_tuning.items()), columns=[\"Metric\", \"After Fine-Tuning\"])\n",
    "\n",
    "\n",
    "\n",
    "# Rename the 'Metric' column to match before merging\n",
    "\n",
    "df_after['Metric'] = df_after['Metric'].apply(lambda x: x.replace('fine_tune_', 'eval_'))\n",
    "\n",
    "\n",
    "\n",
    "# Merge the two DataFrames on the 'Metric' column\n",
    "\n",
    "df_comparison = pd.merge(df_before, df_after, on=\"Metric\")\n",
    "\n",
    "\n",
    "\n",
    "# Print the comparison table using tabulate for better formatting\n",
    "\n",
    "print(\"\\nComparison Results:\")\n",
    "\n",
    "print(tabulate(df_comparison, headers='keys', tablefmt='pipe', showindex=False))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Comparison before and after fine-tuning done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As can be seen, the model performs better in terms of all metrics suitable for classification - accuracy, precision, recall and f1-score after fine-tuning. This is because fine-tuning allows a pre-trained model to adapt to a specific task, using its pre-existing general knowledge and improving its performance on the task by learning task-specific patterns and features. In this case, while the Llama model is highly effective for tasks such as text generation, question answering, language modeling, and other tasks involving natural language inference, they are not specifically designed for classification tasks. So without adding a classification layer and fine-tuning, it wouldn't be suitable for classification task, evident in the model's performance before fine-tuning (after adding classification layer). But fine-tuning greatly improved the model's performance on the SST-2 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Parameters After Fine-Tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T23:09:34.021774Z",
     "iopub.status.busy": "2024-11-21T23:09:34.021411Z",
     "iopub.status.idle": "2024-11-21T23:09:34.107180Z",
     "shell.execute_reply": "2024-11-21T23:09:34.106279Z",
     "shell.execute_reply.started": "2024-11-21T23:09:34.021743Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters for LlamaForSequenceClassification:\n",
      "\n",
      "Parameter Name: model.embed_tokens.weight\n",
      "  Shape: [128256, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 262,668,288\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.0.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.0.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.0.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.0.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.0.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.0.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.0.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.0.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.0.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.1.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.1.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.1.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.1.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.1.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.1.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.1.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.1.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.1.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.2.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.2.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.2.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.2.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.2.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.2.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.2.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.2.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.2.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.3.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.3.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.3.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.3.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.3.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.3.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.3.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.3.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.3.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.4.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.4.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.4.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.4.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.4.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.4.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.4.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.4.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.4.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.5.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.5.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.5.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.5.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.5.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.5.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.5.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.5.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.5.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.6.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.6.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.6.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.6.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.6.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.6.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.6.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.6.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.6.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.7.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.7.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.7.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.7.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.7.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.7.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.7.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.7.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.7.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.8.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.8.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.8.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.8.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.8.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.8.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.8.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.8.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.8.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.9.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.9.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.9.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.9.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.9.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.9.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.9.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.9.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.9.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.10.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.10.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.10.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.10.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.10.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.10.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.10.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.10.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.10.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.11.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.11.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.11.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.11.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.11.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.11.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.11.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.11.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.11.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.12.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.12.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.12.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.12.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.12.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.12.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.12.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.12.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.12.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.13.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.13.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.13.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.13.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.13.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.13.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.13.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.13.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.13.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.14.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.14.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.14.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.14.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.14.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.14.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.14.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.14.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.14.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.15.self_attn.q_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.15.self_attn.k_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.15.self_attn.v_proj.weight\n",
      "  Shape: [512, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 1,048,576\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.15.self_attn.o_proj.weight\n",
      "  Shape: [2048, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 4,194,304\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.15.mlp.gate_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.15.mlp.up_proj.weight\n",
      "  Shape: [8192, 2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.15.mlp.down_proj.weight\n",
      "  Shape: [2048, 8192]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 16,777,216\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.15.input_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.layers.15.post_attention_layernorm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: model.norm.weight\n",
      "  Shape: [2048]\n",
      "  Requires Grad: False\n",
      "  Total Elements: 2,048\n",
      "--------------------------------------------------\n",
      "Parameter Name: score.weight\n",
      "  Shape: [2, 2048]\n",
      "  Requires Grad: True\n",
      "  Total Elements: 4,096\n",
      "--------------------------------------------------\n",
      "\n",
      "Total Parameters: 1,235,818,496\n",
      "Trainable Parameters: 4,096\n",
      "Non-Trainable Parameters: 1,235,814,400\n",
      "Parameters after fine-tuning calculated.\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "\n",
    "def print_model_parameters_tabular(model):\n",
    "\n",
    "    parameters = []\n",
    "\n",
    "    total_params = 0\n",
    "\n",
    "    trainable_params = 0\n",
    "\n",
    "\n",
    "\n",
    "    # Collect model parameter details\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "\n",
    "        num_params = param.numel()\n",
    "\n",
    "        total_params += num_params\n",
    "\n",
    "        if param.requires_grad:\n",
    "\n",
    "            trainable_params += num_params\n",
    "\n",
    "        parameters.append([name, list(param.shape), param.requires_grad, f\"{num_params:,}\"])\n",
    "\n",
    "\n",
    "\n",
    "    # Print parameter details in a more compact, line-by-line format\n",
    "\n",
    "    print(f\"Model Parameters for {type(model).__name__}:\\n\")\n",
    "\n",
    "    for param in parameters:\n",
    "\n",
    "        name, shape, requires_grad, num_elements = param\n",
    "\n",
    "        print(f\"Parameter Name: {name}\")\n",
    "\n",
    "        print(f\"  Shape: {shape}\")\n",
    "\n",
    "        print(f\"  Requires Grad: {requires_grad}\")\n",
    "\n",
    "        print(f\"  Total Elements: {num_elements}\")\n",
    "\n",
    "        print(\"-\" * 50)  # Separator line for clarity\n",
    "\n",
    "    \n",
    "\n",
    "    # Print summary\n",
    "\n",
    "    print(f\"\\nTotal Parameters: {total_params:,}\")\n",
    "\n",
    "    print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "\n",
    "    print(f\"Non-Trainable Parameters: {total_params - trainable_params:,}\")\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "print_model_parameters_tabular(model)\n",
    "\n",
    "print(\"Parameters after fine-tuning calculated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total model parameters after adding the classification layer and fine-tuning: 1235818496"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As can be seen, 4096 extra parameters were added after making the model suitable for classification on adding the classification layer and fine-tuning the model on a classification dataset. (as explained above, the 4096 parameters are due to the addition of the classification layer, not due to fine-tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading the model to Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T23:09:52.915607Z",
     "iopub.status.busy": "2024-11-21T23:09:52.914746Z",
     "iopub.status.idle": "2024-11-21T23:12:31.988696Z",
     "shell.execute_reply": "2024-11-21T23:12:31.987888Z",
     "shell.execute_reply.started": "2024-11-21T23:09:52.915573Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ca78d54e04442096d1e27aa5730e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/jiya14desai/Llama-3.2-1B_fine-tuned_on_classification/commit/1be21fd06ad33e619e8710308964d818e9f93ee7', commit_message='Upload LlamaForSequenceClassification', commit_description='', oid='1be21fd06ad33e619e8710308964d818e9f93ee7', pr_url=None, repo_url=RepoUrl('https://huggingface.co/jiya14desai/Llama-3.2-1B_fine-tuned_on_classification', endpoint='https://huggingface.co', repo_type='model', repo_id='jiya14desai/Llama-3.2-1B_fine-tuned_on_classification'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repository name on Hugging Face Hub (e.g., \"username/model_name\")\n",
    "\n",
    "repo_name = \"jiya14desai/Llama-3.2-1B_fine-tuned_on_classification\"\n",
    "\n",
    "\n",
    "\n",
    "# Push to Hugging Face Hub\n",
    "\n",
    "model.push_to_hub(repo_name)\n",
    "\n",
    "# tokenizer.push_to_hub(repo_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The final fine-tuned model is available on https://huggingface.co/jiya14desai/Llama-3.2-1B_fine-tuned_on_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T06:58:34.919836Z",
     "iopub.status.busy": "2024-11-22T06:58:34.919183Z",
     "iopub.status.idle": "2024-11-22T07:00:45.054204Z",
     "shell.execute_reply": "2024-11-22T07:00:45.053234Z",
     "shell.execute_reply.started": "2024-11-22T06:58:34.919803Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2a152b0c684400884c2df4d6f9500a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/946 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67377b63ecba42fca2611835f9d0c997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e7f36f624f4015b1c4fad4cad779a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4524ac55249643a1a89c2107ca9bcfa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0a5ae8c10a242eba874c4409e6c0d8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: This movie is great! A must-watch.\n",
      "Predicted class: 1\n",
      "\n",
      "Input: This could be the greatest flop movie of all times.\n",
      "Predicted class: 0\n",
      "\n",
      "Input: Average script but good performance by the actors.\n",
      "Predicted class: 1\n",
      "\n",
      "Input: A fun one-time watch.\n",
      "Predicted class: 1\n",
      "\n",
      "Input: Very mid. Forgettable.\n",
      "Predicted class: 0\n",
      "\n",
      "Input: Read the book, don't watch the movie.\n",
      "Predicted class: 0\n",
      "\n",
      "Input: Don't watch if you don't have a good taste in art. Only for admirers of art.\n",
      "Predicted class: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_name = \"jiya14desai/Llama-3.2-1B_fine-tuned_on_classification\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Use the standard <eos> token as the pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.resize_token_embeddings(len(tokenizer))  # Resize embeddings for new pad token\n",
    "\n",
    "# Input texts\n",
    "texts = [\n",
    "    \"This movie is great! A must-watch.\",\n",
    "    \"This could be the greatest flop movie of all times.\",\n",
    "    \"Average script but good performance by the actors.\",\n",
    "    \"A fun one-time watch.\",\n",
    "    \"Very mid. Forgettable.\",\n",
    "    \"Read the book, don't watch the movie.\",\n",
    "    \"Don't watch if you don't have a good taste in art. Only for admirers of art.\"\n",
    "]\n",
    "\n",
    "# Predict for each input\n",
    "for text in texts:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = logits.argmax(dim=-1)\n",
    "    print(f\"Input: {text}\")\n",
    "    print(f\"Predicted class: {predictions.item()}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
